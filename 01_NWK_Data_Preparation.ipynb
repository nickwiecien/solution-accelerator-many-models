{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.7 64-bit ('env': venv)",
   "metadata": {
    "interpreter": {
     "hash": "95f9629f92a95742dc82007aa5e69ca3b1b3071947132bf3b43a4c7c35cf64f4"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set vars for connecting to AML workspace\n",
    "import os\n",
    "\n",
    "subscription_id = os.getenv(\"SUBSCRIPTION_ID\", default=\"\")\n",
    "resource_group = os.getenv(\"RESOURCE_GROUP\", default=\"\")\n",
    "workspace_name = os.getenv(\"WORKSPACE_NAME\", default=\"\")\n",
    "workspace_region = os.getenv(\"WORKSPACE_REGION\", default=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Workspace configuration succeeded. Skip the workspace creation steps below\n"
     ]
    }
   ],
   "source": [
    "#Create AML workspace connection\n",
    "from azureml.core import Workspace\n",
    "\n",
    "try:\n",
    "    ws = Workspace(subscription_id=subscription_id, \n",
    "                   resource_group=resource_group, \n",
    "                   workspace_name=workspace_name)\n",
    "    print(\"Workspace configuration succeeded. Skip the workspace creation steps below\")\n",
    "except:\n",
    "    print(\"Workspace does not exist. Creating workspace\")\n",
    "    ws = Workspace.create(name=workspace_name, subscription_id=subscription_id, resource_group=resource_group,\n",
    "                            location=workspace_region, create_resource_group=True, sku='enterprise', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'id': '/subscriptions/f3e38aaa-dd9c-4f17-95c1-ef3ff472da61/resourceGroups/nwk-aml-nov20-rg/providers/Microsoft.MachineLearningServices/workspaces/nwk-aml-nov20', 'name': 'nwk-aml-nov20', 'location': 'eastus2', 'type': 'Microsoft.MachineLearningServices/workspaces', 'sku': 'Basic', 'workspaceid': '90700d52-202e-41a2-a5ea-297b8d0ebff8', 'description': '', 'friendlyName': 'nwk-aml-nov20', 'creationTime': '2020-11-17T16:30:09.3944271+00:00', 'containerRegistry': '/subscriptions/f3e38aaa-dd9c-4f17-95c1-ef3ff472da61/resourceGroups/nwk-aml-nov20-rg/providers/Microsoft.ContainerRegistry/registries/90700d52202e41a2a5ea297b8d0ebff8', 'keyVault': '/subscriptions/f3e38aaa-dd9c-4f17-95c1-ef3ff472da61/resourcegroups/nwk-aml-nov20-rg/providers/microsoft.keyvault/vaults/kv5nmqnmqqzbyzq', 'applicationInsights': '/subscriptions/f3e38aaa-dd9c-4f17-95c1-ef3ff472da61/resourcegroups/nwk-aml-nov20-rg/providers/microsoft.insights/components/ai5nmqnmqqzbyzq', 'identityPrincipalId': '6ccd454e-22e4-4ec3-b55a-73de80995dcc', 'identityTenantId': '72f988bf-86f1-41af-91ab-2d7cd011db47', 'identityType': 'SystemAssigned', 'storageAccount': '/subscriptions/f3e38aaa-dd9c-4f17-95c1-ef3ff472da61/resourcegroups/nwk-aml-nov20-rg/providers/microsoft.storage/storageaccounts/sa5nmqnmqqzbyzq', 'hbiWorkspace': False, 'discoveryUrl': 'https://eastus2.experiments.azureml.net/discovery', 'notebookInfo': {'fqdn': 'ml-nwk-aml-nov20-eastus2-90700d52-202e-41a2-a5ea-297b8d0ebff8.notebooks.azure.net', 'resource_id': '929d4d5a7f3149938f7b2151a0c371b1'}}\n"
     ]
    }
   ],
   "source": [
    "#Print AML workspace details and write config file\n",
    "print(ws.get_details())\n",
    "\n",
    "ws.write_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Found an existing cluster, using it instead.\n"
     ]
    }
   ],
   "source": [
    "# Choose a name for your CPU cluster\n",
    "cpu_cluster_name = 'cpucluster'\n",
    "\n",
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "# Verify that cluster does not exist already\n",
    "try:\n",
    "    cpu_cluster = ComputeTarget(workspace=ws, name=cpu_cluster_name)\n",
    "    print('Found an existing cluster, using it instead.')\n",
    "except ComputeTargetException:\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_D13_V2',\n",
    "                                                           min_nodes=0,\n",
    "                                                           max_nodes=10)\n",
    "    cpu_cluster = ComputeTarget.create(ws, cpu_cluster_name, compute_config)\n",
    "    cpu_cluster.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create new experiment for pipeline submission\n",
    "from azureml.core import Experiment\n",
    "\n",
    "experiment = Experiment(ws, 'sample-dataprep-pipeline')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Specify folder which will contain data preparation scripts\n",
    "scripts_folder = './dataprep/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write script to be run in pipeline. Should gather data, upload to AML associated datastore,\n",
    "#create and register datasets\n",
    "%%writefile $scripts_folder/gather_data.py\n",
    "import pandas as pd\n",
    "import requests\n",
    "from csv import reader\n",
    "import os\n",
    "import argparse\n",
    "import json\n",
    "from azureml.core import Run, Workspace, Datastore, Dataset, Experiment\n",
    "import snowflake.connector\n",
    "\n",
    "parser = argparse.ArgumentParser(\"Aggregate Data\")\n",
    "\n",
    "parser.add_argument(\"--target_path\", type=str, help=\"Target path for data upload\")\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "#Get current run\n",
    "current_run = Run.get_context()\n",
    "#Get associated AML workspace\n",
    "ws = current_run.experiment.workspace\n",
    "#Get default datastore - used to upload data\n",
    "datastore = ws.get_default_datastore()\n",
    "\n",
    "#Target path is passed as a variable argument (can be timestamped)\n",
    "target_path = args.target_path\n",
    "\n",
    "#Pull sample dataset and add to pandas dataframe\n",
    "r = requests.get('https://dprepdata.blob.core.windows.net/demo/Titanic.csv')\n",
    "rows = r.text.split('\\r\\n')\n",
    "formatted_rows = []\n",
    "for row in rows:\n",
    "    read = reader([row], skipinitialspace=True)\n",
    "    vals = [x for x in read]\n",
    "    formatted_rows.append(vals[0])\n",
    "df = pd.DataFrame(formatted_rows[1:], columns=formatted_rows[0])\n",
    "\n",
    "#Partition source dataframe based on values in 'Embarked' column\n",
    "df_s = df[df['Embarked']=='S']\n",
    "df_c = df[df['Embarked']=='C']\n",
    "df_q = df[df['Embarked']=='Q']\n",
    "\n",
    "#Write partitioned dataframes to files in processed data\n",
    "df_s.to_csv('./processed/sourcedata_s.csv', index=False)\n",
    "df_c.to_csv('./processed/sourcedata_c.csv', index=False)\n",
    "df_q.to_csv('./processed/sourcedata_q.csv', index=False)\n",
    "\n",
    "#Upload processed directory to default datastore\n",
    "datastore.upload(src_dir='./processed', target_path=target_path, overwrite=True)\n",
    "\n",
    "from azureml.core.dataset import Dataset\n",
    "\n",
    "# Create file datasets\n",
    "ds_train = Dataset.File.from_files(path=datastore.path(target_path), validate=False)\n",
    "\n",
    "# Register the file datasets\n",
    "dataset_name = 'etf_data'\n",
    "train_dataset_name = dataset_name + '_train'\n",
    "ds_train.register(ws, train_dataset_name, create_new_version=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "20201118102734\n"
     ]
    }
   ],
   "source": [
    "#Create timestamp\n",
    "#Note: all files are uploaded to timestamped subdir in AML datastore\n",
    "import time\n",
    "\n",
    "secondsSinceEpoch = time.time()\n",
    "timeObj = time.localtime(secondsSinceEpoch)\n",
    "\n",
    "timestamp = ('%d%d%d%d%d%d' % (\n",
    "timeObj.tm_year, timeObj.tm_mon, timeObj.tm_mday, timeObj.tm_hour, timeObj.tm_min, timeObj.tm_sec))\n",
    "\n",
    "print(timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create PythonScriptStep and associated run configuration\n",
    "from azureml.core.runconfig import RunConfiguration\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "from azureml.core.runconfig import DEFAULT_CPU_IMAGE\n",
    "from azureml.pipeline.core import Pipeline\n",
    "from azureml.pipeline.steps import PythonScriptStep\n",
    "from azureml.pipeline.core import PipelineParameter\n",
    "\n",
    "# create a new runconfig object\n",
    "run_config = RunConfiguration()\n",
    "\n",
    "# enable Docker \n",
    "run_config.environment.docker.enabled = True\n",
    "\n",
    "# set Docker base image to the default CPU-based image\n",
    "# run_config.environment.docker.base_image = DEFAULT_CPU_IMAGE\n",
    "\n",
    "# use conda_dependencies.yml to create a conda environment in the Docker image for execution\n",
    "# run_config.environment.python.user_managed_dependencies = False\n",
    "\n",
    "# specify CondaDependencies obj\n",
    "# run_config.environment.python.conda_dependencies = CondaDependencies.create(conda_packages=['requests', 'pandas'])\n",
    "\n",
    "#Create pipeline parameter\n",
    "pipeline_param = PipelineParameter(name=\"target_path\", default_value=\"source_data_0000\")\n",
    "\n",
    "#aml-pipelines-with-data-dependency-steps.ipynb\n",
    "aggregateDataStep = PythonScriptStep(\n",
    "    script_name=\"gather_data.py\", \n",
    "    arguments=[\"--target_path\", pipeline_param],\n",
    "    compute_target=cpu_cluster, \n",
    "    source_directory=scripts_folder,\n",
    "    runconfig=run_config\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "reml.net/history/v1.0/private/subscriptions/f3e38aaa-dd9c-4f17-95c1-ef3ff472da61/resourceGroups/nwk-aml-nov20-rg/providers/Microsoft.MachineLearningServices/workspaces/nwk-aml-nov20/runs/9af0fbc2-04a3-4035-9dd3-200e14bf88a8/spans\n",
      ">>>   2020/11/18 16:31:38 Container ssh is not required for job type.\n",
      ">>>   2020/11/18 16:31:38 Starting docker container succeeded.\n",
      ">>>   2020/11/18 16:31:38 Starting docker container succeeded.\n",
      ">>>   2020/11/18 16:31:38 runSpecialJobTask: os.GetEnv constants.StdouterrDir: /mnt/batch/tasks/shared/LS_root/jobs/nwk-aml-nov20/azureml/9af0fbc2-04a3-4035-9dd3-200e14bf88a8/mounts/workspaceblobstore/azureml/9af0fbc2-04a3-4035-9dd3-200e14bf88a8/azureml_compute_logs\n",
      ">>>   2020/11/18 16:31:38 runSpecialJobTask: Raw cmd for preparation is passed is: /azureml-envs/azureml_927604824676dd59c4a4fe663f49cc34/bin/python /mnt/batch/tasks/shared/LS_root/jobs/nwk-aml-nov20/azureml/9af0fbc2-04a3-4035-9dd3-200e14bf88a8/mounts/workspaceblobstore/azureml/9af0fbc2-04a3-4035-9dd3-200e14bf88a8-setup/job_prep.py --snapshots '[{\"Id\":\"08ab1103-5878-43c4-bb37-5282e6fecc61\",\"PathStack\":[\".\"],\"SnapshotEntityId\":null}]'\n",
      ">>>   2020/11/18 16:31:38 runSpecialJobTask: stdout path for preparation is passed is: /mnt/batch/tasks/shared/LS_root/jobs/nwk-aml-nov20/azureml/9af0fbc2-04a3-4035-9dd3-200e14bf88a8/mounts/workspaceblobstore/azureml/9af0fbc2-04a3-4035-9dd3-200e14bf88a8/azureml_compute_logs/65_job_prep-tvmps_9d43444593d083f8bc91d5542fdb04798f3ac665665e5f7a79e06585ddfdcf87_d.txt\n",
      ">>>   2020/11/18 16:31:38 runSpecialJobTask: stderr path for preparation is passed is: /mnt/batch/tasks/shared/LS_root/jobs/nwk-aml-nov20/azureml/9af0fbc2-04a3-4035-9dd3-200e14bf88a8/mounts/workspaceblobstore/azureml/9af0fbc2-04a3-4035-9dd3-200e14bf88a8/azureml_compute_logs/65_job_prep-tvmps_9d43444593d083f8bc91d5542fdb04798f3ac665665e5f7a79e06585ddfdcf87_d.txt\n",
      ">>>   2020/11/18 16:31:38 native cmd: cd /mnt/batch/tasks/shared/LS_root/jobs/nwk-aml-nov20/azureml/9af0fbc2-04a3-4035-9dd3-200e14bf88a8/mounts/workspaceblobstore/azureml/9af0fbc2-04a3-4035-9dd3-200e14bf88a8;/azureml-envs/azureml_927604824676dd59c4a4fe663f49cc34/bin/python /mnt/batch/tasks/shared/LS_root/jobs/nwk-aml-nov20/azureml/9af0fbc2-04a3-4035-9dd3-200e14bf88a8/mounts/workspaceblobstore/azureml/9af0fbc2-04a3-4035-9dd3-200e14bf88a8-setup/job_prep.py --snapshots '[{\"Id\":\"08ab1103-5878-43c4-bb37-5282e6fecc61\",\"PathStack\":[\".\"],\"SnapshotEntityId\":null}]'\n",
      ">>>   2020/11/18 16:31:38 runSpecialJobTask: commons.GetOsPlatform(): ubuntu\n",
      ">>>   2020/11/18 16:31:38 runSpecialJobTask: Running cmd: /usr/bin/docker exec -t 9af0fbc2-04a3-4035-9dd3-200e14bf88a8 bash -c if [ -f ~/.bashrc ]; then PS1_back=$PS1; PS1='$'; . ~/.bashrc; PS1=$PS1_back; fi;PATH=$PATH:$AZ_BATCH_NODE_STARTUP_DIR/wd/;cd /mnt/batch/tasks/shared/LS_root/jobs/nwk-aml-nov20/azureml/9af0fbc2-04a3-4035-9dd3-200e14bf88a8/mounts/workspaceblobstore/azureml/9af0fbc2-04a3-4035-9dd3-200e14bf88a8;/azureml-envs/azureml_927604824676dd59c4a4fe663f49cc34/bin/python /mnt/batch/tasks/shared/LS_root/jobs/nwk-aml-nov20/azureml/9af0fbc2-04a3-4035-9dd3-200e14bf88a8/mounts/workspaceblobstore/azureml/9af0fbc2-04a3-4035-9dd3-200e14bf88a8-setup/job_prep.py --snapshots '[{\"Id\":\"08ab1103-5878-43c4-bb37-5282e6fecc61\",\"PathStack\":[\".\"],\"SnapshotEntityId\":null}]'\n",
      ">>>   2020/11/18 16:31:43 runSpecialJobTask: job preparation exited with code 0 and err <nil>\n",
      ">>>   \n",
      ">>>   2020/11/18 16:31:43 runSpecialJobTask: preparation: [2020-11-18T16:31:39.348524] Entering job preparation.\n",
      ">>>   2020/11/18 16:31:43 runSpecialJobTask: preparation: [2020-11-18T16:31:40.424221] Starting job preparation.\n",
      ">>>   2020/11/18 16:31:43 runSpecialJobTask: preparation: [2020-11-18T16:31:40.424259] Extracting the control code.\n",
      ">>>   2020/11/18 16:31:43 runSpecialJobTask: preparation: [2020-11-18T16:31:40.450407] fetching and extracting the control code on master node.\n",
      ">>>   2020/11/18 16:31:43 runSpecialJobTask: preparation: [2020-11-18T16:31:40.450466] Starting extract_project.\n",
      ">>>   2020/11/18 16:31:43 runSpecialJobTask: preparation: [2020-11-18T16:31:40.450532] Starting to extract zip file.\n",
      ">>>   2020/11/18 16:31:43 runSpecialJobTask: preparation: [2020-11-18T16:31:41.297831] Finished extracting zip file.\n",
      ">>>   2020/11/18 16:31:43 runSpecialJobTask: preparation: [2020-11-18T16:31:41.482743] Using urllib.request Python 3.0 or later\n",
      ">>>   2020/11/18 16:31:43 runSpecialJobTask: preparation: [2020-11-18T16:31:41.482818] Start fetching snapshots.\n",
      ">>>   2020/11/18 16:31:43 runSpecialJobTask: preparation: [2020-11-18T16:31:41.482869] Start fetching snapshot.\n",
      ">>>   2020/11/18 16:31:43 runSpecialJobTask: preparation: [2020-11-18T16:31:41.482888] Retrieving project from snapshot: 08ab1103-5878-43c4-bb37-5282e6fecc61\n",
      ">>>   2020/11/18 16:31:43 runSpecialJobTask: preparation: Starting the daemon thread to refresh tokens in background for process with pid = 53\n",
      ">>>   2020/11/18 16:31:43 runSpecialJobTask: preparation: [2020-11-18T16:31:42.574195] Finished fetching snapshot.\n",
      ">>>   2020/11/18 16:31:43 runSpecialJobTask: preparation: [2020-11-18T16:31:42.574247] Finished fetching snapshots.\n",
      ">>>   2020/11/18 16:31:43 runSpecialJobTask: preparation: [2020-11-18T16:31:42.574267] Finished extract_project.\n",
      ">>>   2020/11/18 16:31:43 runSpecialJobTask: preparation: [2020-11-18T16:31:42.587743] Finished fetching and extracting the control code.\n",
      ">>>   2020/11/18 16:31:43 runSpecialJobTask: preparation: [2020-11-18T16:31:42.591788] downloadDataStore - Download from datastores if requested.\n",
      ">>>   2020/11/18 16:31:43 runSpecialJobTask: preparation: [2020-11-18T16:31:42.592687] Start run_history_prep.\n",
      ">>>   2020/11/18 16:31:43 runSpecialJobTask: preparation: [2020-11-18T16:31:42.654193] Entering context manager injector.\n",
      ">>>   2020/11/18 16:31:43 runSpecialJobTask: preparation: [2020-11-18T16:31:43.347184] downloadDataStore completed\n",
      ">>>   2020/11/18 16:31:43 runSpecialJobTask: preparation: [2020-11-18T16:31:43.350720] Job preparation is complete.\n",
      ">>>   2020/11/18 16:31:44 All App Insights Logs was send successfully\n",
      ">>>   2020/11/18 16:31:44 Process Exiting with Code:  0\n",
      ">>>   \n",
      "2020-11-18T16:31:44Z 127.0.0.1 slots=8 max-slots=8\n",
      "2020-11-18T16:31:44Z launching Custom job\n",
      "\n",
      "Streaming azureml-logs/70_driver_log.txt\n",
      "========================================\n",
      "2020/11/18 16:31:44 Attempt 1 of http call to http://10.0.0.5:16384/sendlogstoartifacts/info\n",
      "2020/11/18 16:31:44 Attempt 1 of http call to http://10.0.0.5:16384/sendlogstoartifacts/status\n",
      "[2020-11-18T16:31:46.067252] Entering context manager injector.\n",
      "[context_manager_injector.py] Command line Options: Namespace(inject=['ProjectPythonPath:context_managers.ProjectPythonPath', 'RunHistory:context_managers.RunHistory', 'TrackUserError:context_managers.TrackUserError', 'UserExceptions:context_managers.UserExceptions'], invocation=['gather_data.py', '--target_path', 'source_data_20201118102734'])\n",
      "Script type = None\n",
      "Starting the daemon thread to refresh tokens in background for process with pid = 110\n",
      "Entering Run History Context Manager.\n",
      "Current directory:  /mnt/batch/tasks/shared/LS_root/jobs/nwk-aml-nov20/azureml/9af0fbc2-04a3-4035-9dd3-200e14bf88a8/mounts/workspaceblobstore/azureml/9af0fbc2-04a3-4035-9dd3-200e14bf88a8\n",
      "Preparing to call script [ gather_data.py ] with arguments: ['--target_path', 'source_data_20201118102734']\n",
      "After variable expansion, calling script [ gather_data.py ] with arguments: ['--target_path', 'source_data_20201118102734']\n",
      "\n",
      "Uploading an estimated of 3 files\n",
      "Uploading ./processed/sourcedata_q.csv\n",
      "Uploaded ./processed/sourcedata_q.csv, 1 files out of an estimated total of 3\n",
      "Uploading ./processed/sourcedata_c.csv\n",
      "Uploaded ./processed/sourcedata_c.csv, 2 files out of an estimated total of 3\n",
      "Uploading ./processed/sourcedata_s.csv\n",
      "Uploaded ./processed/sourcedata_s.csv, 3 files out of an estimated total of 3\n",
      "Uploaded 3 files\n",
      "Starting the daemon thread to refresh tokens in background for process with pid = 110\n",
      "\n",
      "\n",
      "[2020-11-18T16:31:58.694066] The experiment completed successfully. Finalizing run...\n",
      "Cleaning up all outstanding Run operations, waiting 900.0 seconds\n",
      "2 items cleaning up...\n",
      "Cleanup took 0.14677929878234863 seconds\n",
      "[2020-11-18T16:31:59.525467] Finished context manager injector.\n",
      "\n",
      "Streaming azureml-logs/75_job_post-tvmps_9d43444593d083f8bc91d5542fdb04798f3ac665665e5f7a79e06585ddfdcf87_d.txt\n",
      "===============================================================================================================\n",
      "[2020-11-18T16:32:02.106829] Entering job release\n",
      "[2020-11-18T16:32:03.046541] Starting job release\n",
      "[2020-11-18T16:32:03.052163] Logging experiment finalizing status in history service.\n",
      "[2020-11-18T16:32:03.052362] job release stage : upload_datastore starting...\n",
      "Starting the daemon thread to refresh tokens in background for process with pid = 324\n",
      "[2020-11-18T16:32:03.053314] job release stage : start importing azureml.history._tracking in run_history_release.\n",
      "[2020-11-18T16:32:03.053678] job release stage : copy_batchai_cached_logs starting...\n",
      "[2020-11-18T16:32:03.053830] job release stage : execute_job_release starting...\n",
      "[2020-11-18T16:32:03.054115] job release stage : copy_batchai_cached_logs completed...\n",
      "[2020-11-18T16:32:03.063838] Entering context manager injector.\n",
      "[2020-11-18T16:32:03.065758] job release stage : upload_datastore completed...\n",
      "[2020-11-18T16:32:03.388746] job release stage : send_run_telemetry starting...\n",
      "[2020-11-18T16:32:03.507773] job release stage : execute_job_release completed...\n",
      "[2020-11-18T16:32:04.827067] job release stage : send_run_telemetry completed...\n",
      "[2020-11-18T16:32:04.827429] Job release is complete\n",
      "\n",
      "StepRun(gather_data.py) Execution Summary\n",
      "==========================================\n",
      "StepRun( gather_data.py ) Status: Finished\n",
      "{'runId': '9af0fbc2-04a3-4035-9dd3-200e14bf88a8', 'target': 'cpucluster', 'status': 'Completed', 'startTimeUtc': '2020-11-18T16:30:48.862409Z', 'endTimeUtc': '2020-11-18T16:32:13.780988Z', 'properties': {'azureml.runsource': 'azureml.StepRun', 'ContentSnapshotId': '08ab1103-5878-43c4-bb37-5282e6fecc61', 'StepType': 'PythonScriptStep', 'ComputeTargetType': 'AmlCompute', 'azureml.moduleid': '061f2b23-e6a5-49e7-8710-f453980fdf10', 'azureml.nodeid': '5123ebe5', 'azureml.pipelinerunid': 'cab8ad6f-0844-4d4c-998d-e8b812899642', '_azureml.ComputeTargetType': 'amlcompute', 'ProcessInfoFile': 'azureml-logs/process_info.json', 'ProcessStatusFile': 'azureml-logs/process_status.json'}, 'inputDatasets': [], 'outputDatasets': [{'identifier': {'savedId': '930691ab-9366-4b65-b747-0e587c9924cf', 'registeredId': '83660115-8397-4d5d-adcb-3bf7327e5433', 'registeredVersion': '6'}, 'outputType': 'Reference', 'dataset': {\n",
      "  \"source\": [\n",
      "    \"('workspaceblobstore', 'source_data_20201118102734')\"\n",
      "  ],\n",
      "  \"definition\": [\n",
      "    \"GetDatastoreFiles\"\n",
      "  ],\n",
      "  \"registration\": {\n",
      "    \"id\": \"930691ab-9366-4b65-b747-0e587c9924cf\",\n",
      "    \"name\": \"etf_data_train\",\n",
      "    \"version\": 6,\n",
      "    \"workspace\": \"Workspace.create(name='nwk-aml-nov20', subscription_id='f3e38aaa-dd9c-4f17-95c1-ef3ff472da61', resource_group='nwk-aml-nov20-rg')\"\n",
      "  }\n",
      "}}], 'runDefinition': {'script': 'gather_data.py', 'useAbsolutePath': False, 'arguments': ['--target_path', '$AML_PARAMETER_target_path'], 'sourceDirectoryDataStore': None, 'framework': 'Python', 'communicator': 'None', 'target': 'cpucluster', 'dataReferences': {}, 'data': {}, 'outputData': {}, 'jobName': None, 'maxRunDurationSeconds': None, 'nodeCount': 1, 'priority': None, 'environment': {'name': 'Experiment sample-dataprep-pipeline Environment', 'version': 'Autosave_2020-11-18T16:27:54Z_43faa95c', 'python': {'interpreterPath': 'python', 'userManagedDependencies': False, 'condaDependencies': {'channels': ['anaconda', 'conda-forge'], 'dependencies': ['python=3.6.2', {'pip': ['azureml-defaults']}, 'requests', 'pandas'], 'name': 'azureml_927604824676dd59c4a4fe663f49cc34'}, 'baseCondaEnvironment': None}, 'environmentVariables': {'EXAMPLE_ENV_VAR': 'EXAMPLE_VALUE', 'AML_PARAMETER_target_path': 'source_data_20201118102734'}, 'docker': {'baseImage': 'mcr.microsoft.com/azureml/intelmpi2018.3-ubuntu16.04:20200821.v1', 'platform': {'os': 'Linux', 'architecture': 'amd64'}, 'baseDockerfile': None, 'baseImageRegistry': {'address': None, 'username': None, 'password': None}, 'enabled': True, 'arguments': []}, 'spark': {'repositories': [], 'packages': [], 'precachePackages': True}, 'inferencingStackVersion': None}, 'history': {'outputCollection': True, 'directoriesToWatch': ['logs'], 'enableMLflowTracking': True, 'snapshotProject': True}, 'spark': {'configuration': {'spark.app.name': 'Azure ML Experiment', 'spark.yarn.maxAppAttempts': '1'}}, 'parallelTask': {'maxRetriesPerWorker': 0, 'workerCountPerNode': 1, 'terminalExitCodes': None, 'configuration': {}}, 'amlCompute': {'name': None, 'vmSize': None, 'retainCluster': False, 'clusterMaxNodeCount': 1}, 'aiSuperComputer': {'instanceType': None, 'frameworkImage': None, 'imageVersion': None, 'location': None}, 'tensorflow': {'workerCount': 1, 'parameterServerCount': 1}, 'mpi': {'processCountPerNode': 1}, 'hdi': {'yarnDeployMode': 'Cluster'}, 'containerInstance': {'region': None, 'cpuCores': 2.0, 'memoryGb': 3.5}, 'exposedPorts': None, 'docker': {'useDocker': True, 'sharedVolumes': True, 'shmSize': '2g', 'arguments': []}, 'cmk8sCompute': {'configuration': {}}}, 'logFiles': {'azureml-logs/55_azureml-execution-tvmps_9d43444593d083f8bc91d5542fdb04798f3ac665665e5f7a79e06585ddfdcf87_d.txt': 'https://sa5nmqnmqqzbyzq.blob.core.windows.net/azureml/ExperimentRun/dcid.9af0fbc2-04a3-4035-9dd3-200e14bf88a8/azureml-logs/55_azureml-execution-tvmps_9d43444593d083f8bc91d5542fdb04798f3ac665665e5f7a79e06585ddfdcf87_d.txt?sv=2019-02-02&sr=b&sig=o829XARGbJyt4%2BfIHE5Qsdm7t%2FeFZ1g67uCGErH3hMI%3D&st=2020-11-18T16%3A22%3A07Z&se=2020-11-19T00%3A32%3A07Z&sp=r', 'azureml-logs/65_job_prep-tvmps_9d43444593d083f8bc91d5542fdb04798f3ac665665e5f7a79e06585ddfdcf87_d.txt': 'https://sa5nmqnmqqzbyzq.blob.core.windows.net/azureml/ExperimentRun/dcid.9af0fbc2-04a3-4035-9dd3-200e14bf88a8/azureml-logs/65_job_prep-tvmps_9d43444593d083f8bc91d5542fdb04798f3ac665665e5f7a79e06585ddfdcf87_d.txt?sv=2019-02-02&sr=b&sig=Mqq%2BAXVV%2BXpqbrOuid%2BdqwtOzclt4JyvZ91lQfUlKvs%3D&st=2020-11-18T16%3A22%3A07Z&se=2020-11-19T00%3A32%3A07Z&sp=r', 'azureml-logs/70_driver_log.txt': 'https://sa5nmqnmqqzbyzq.blob.core.windows.net/azureml/ExperimentRun/dcid.9af0fbc2-04a3-4035-9dd3-200e14bf88a8/azureml-logs/70_driver_log.txt?sv=2019-02-02&sr=b&sig=2OLJkp%2Be6rCEFv41HL7gO5M15IebeHBirqggpynBGH0%3D&st=2020-11-18T16%3A22%3A07Z&se=2020-11-19T00%3A32%3A07Z&sp=r', 'azureml-logs/75_job_post-tvmps_9d43444593d083f8bc91d5542fdb04798f3ac665665e5f7a79e06585ddfdcf87_d.txt': 'https://sa5nmqnmqqzbyzq.blob.core.windows.net/azureml/ExperimentRun/dcid.9af0fbc2-04a3-4035-9dd3-200e14bf88a8/azureml-logs/75_job_post-tvmps_9d43444593d083f8bc91d5542fdb04798f3ac665665e5f7a79e06585ddfdcf87_d.txt?sv=2019-02-02&sr=b&sig=EU9iDa8H7g20O3E%2BOkSIxJ6A%2BDUIDYGKrKwMk3sGk%2Bs%3D&st=2020-11-18T16%3A22%3A07Z&se=2020-11-19T00%3A32%3A07Z&sp=r', 'azureml-logs/process_info.json': 'https://sa5nmqnmqqzbyzq.blob.core.windows.net/azureml/ExperimentRun/dcid.9af0fbc2-04a3-4035-9dd3-200e14bf88a8/azureml-logs/process_info.json?sv=2019-02-02&sr=b&sig=LKBu6BFGGfmzSzNJZ1zzS1hi8EBkryky4JOokJ4iBuU%3D&st=2020-11-18T16%3A22%3A07Z&se=2020-11-19T00%3A32%3A07Z&sp=r', 'azureml-logs/process_status.json': 'https://sa5nmqnmqqzbyzq.blob.core.windows.net/azureml/ExperimentRun/dcid.9af0fbc2-04a3-4035-9dd3-200e14bf88a8/azureml-logs/process_status.json?sv=2019-02-02&sr=b&sig=5sAFZZPb3%2B%2FNwFVcuZp6%2FFGwY%2F%2FaANOSvkgMMIt7bdo%3D&st=2020-11-18T16%3A22%3A07Z&se=2020-11-19T00%3A32%3A07Z&sp=r', 'logs/azureml/110_azureml.log': 'https://sa5nmqnmqqzbyzq.blob.core.windows.net/azureml/ExperimentRun/dcid.9af0fbc2-04a3-4035-9dd3-200e14bf88a8/logs/azureml/110_azureml.log?sv=2019-02-02&sr=b&sig=UyZKPCvM9SDSppbLP17mBWjN4SDJKw%2BYVClJ4zG4FdQ%3D&st=2020-11-18T16%3A22%3A07Z&se=2020-11-19T00%3A32%3A07Z&sp=r', 'logs/azureml/dataprep/backgroundProcess.log': 'https://sa5nmqnmqqzbyzq.blob.core.windows.net/azureml/ExperimentRun/dcid.9af0fbc2-04a3-4035-9dd3-200e14bf88a8/logs/azureml/dataprep/backgroundProcess.log?sv=2019-02-02&sr=b&sig=THUmQ5tcvco2mlI3UuYpf7Vbuheh0jYmSimyIpgHZm4%3D&st=2020-11-18T16%3A22%3A07Z&se=2020-11-19T00%3A32%3A07Z&sp=r', 'logs/azureml/dataprep/engine_spans_l_e6e7edb2-37fb-4f53-ae7d-ac06b664d403.jsonl': 'https://sa5nmqnmqqzbyzq.blob.core.windows.net/azureml/ExperimentRun/dcid.9af0fbc2-04a3-4035-9dd3-200e14bf88a8/logs/azureml/dataprep/engine_spans_l_e6e7edb2-37fb-4f53-ae7d-ac06b664d403.jsonl?sv=2019-02-02&sr=b&sig=91Liiv2N6OvxXykMFChhc7SQx1khsTqDOs2neo%2FiNzQ%3D&st=2020-11-18T16%3A22%3A07Z&se=2020-11-19T00%3A32%3A07Z&sp=r', 'logs/azureml/dataprep/python_span_l_e6e7edb2-37fb-4f53-ae7d-ac06b664d403.jsonl': 'https://sa5nmqnmqqzbyzq.blob.core.windows.net/azureml/ExperimentRun/dcid.9af0fbc2-04a3-4035-9dd3-200e14bf88a8/logs/azureml/dataprep/python_span_l_e6e7edb2-37fb-4f53-ae7d-ac06b664d403.jsonl?sv=2019-02-02&sr=b&sig=AHSoDlXto5%2BfqFi5y%2BiUWrAGVBjjtPVjqOfbtiskFyg%3D&st=2020-11-18T16%3A22%3A07Z&se=2020-11-19T00%3A32%3A07Z&sp=r', 'logs/azureml/executionlogs.txt': 'https://sa5nmqnmqqzbyzq.blob.core.windows.net/azureml/ExperimentRun/dcid.9af0fbc2-04a3-4035-9dd3-200e14bf88a8/logs/azureml/executionlogs.txt?sv=2019-02-02&sr=b&sig=f1oUdUUawDWp3Ye1kaS0CBopshaJ6y3eQeRIBL7i40U%3D&st=2020-11-18T16%3A22%3A07Z&se=2020-11-19T00%3A32%3A07Z&sp=r', 'logs/azureml/job_prep_azureml.log': 'https://sa5nmqnmqqzbyzq.blob.core.windows.net/azureml/ExperimentRun/dcid.9af0fbc2-04a3-4035-9dd3-200e14bf88a8/logs/azureml/job_prep_azureml.log?sv=2019-02-02&sr=b&sig=guUFiXsHCaxx5pA%2FpWA52lBcIpHSwzNfXQCCPSvw%2Bkc%3D&st=2020-11-18T16%3A22%3A07Z&se=2020-11-19T00%3A32%3A07Z&sp=r', 'logs/azureml/job_release_azureml.log': 'https://sa5nmqnmqqzbyzq.blob.core.windows.net/azureml/ExperimentRun/dcid.9af0fbc2-04a3-4035-9dd3-200e14bf88a8/logs/azureml/job_release_azureml.log?sv=2019-02-02&sr=b&sig=BHK9kWpR6hdftMmVrhLKb8u%2BWVxtyf%2B0SGSPQhuDSqA%3D&st=2020-11-18T16%3A22%3A07Z&se=2020-11-19T00%3A32%3A07Z&sp=r', 'logs/azureml/stderrlogs.txt': 'https://sa5nmqnmqqzbyzq.blob.core.windows.net/azureml/ExperimentRun/dcid.9af0fbc2-04a3-4035-9dd3-200e14bf88a8/logs/azureml/stderrlogs.txt?sv=2019-02-02&sr=b&sig=v8QbWXcptIzrcEHa7W7KuBSARJhLb6s%2BhO4NXxKEb98%3D&st=2020-11-18T16%3A22%3A07Z&se=2020-11-19T00%3A32%3A07Z&sp=r', 'logs/azureml/stdoutlogs.txt': 'https://sa5nmqnmqqzbyzq.blob.core.windows.net/azureml/ExperimentRun/dcid.9af0fbc2-04a3-4035-9dd3-200e14bf88a8/logs/azureml/stdoutlogs.txt?sv=2019-02-02&sr=b&sig=ln16g04bUq1JlM6%2FNG1wMq50pM%2FUBmid6Bt7r11h4wI%3D&st=2020-11-18T16%3A22%3A07Z&se=2020-11-19T00%3A32%3A07Z&sp=r'}}\n",
      "\n",
      "\n",
      "\n",
      "PipelineRun Execution Summary\n",
      "==============================\n",
      "PipelineRun Status: Finished\n",
      "{'runId': 'cab8ad6f-0844-4d4c-998d-e8b812899642', 'status': 'Completed', 'startTimeUtc': '2020-11-18T16:27:44.507949Z', 'endTimeUtc': '2020-11-18T16:32:29.581049Z', 'properties': {'azureml.runsource': 'azureml.PipelineRun', 'runSource': 'SDK', 'runType': 'SDK', 'azureml.parameters': '{\"target_path\":\"source_data_20201118102734\"}'}, 'inputDatasets': [], 'logFiles': {'logs/azureml/executionlogs.txt': 'https://sa5nmqnmqqzbyzq.blob.core.windows.net/azureml/ExperimentRun/dcid.cab8ad6f-0844-4d4c-998d-e8b812899642/logs/azureml/executionlogs.txt?sv=2019-02-02&sr=b&sig=ryFSzutkaoOdfp2OdxzZpPJSjF2UW%2Bp%2BQ9VL3RgnykI%3D&st=2020-11-18T16%3A22%3A33Z&se=2020-11-19T00%3A32%3A33Z&sp=r', 'logs/azureml/stderrlogs.txt': 'https://sa5nmqnmqqzbyzq.blob.core.windows.net/azureml/ExperimentRun/dcid.cab8ad6f-0844-4d4c-998d-e8b812899642/logs/azureml/stderrlogs.txt?sv=2019-02-02&sr=b&sig=6cdZkq2mqpZjuvhEdDAdUEOxOgRHjOkQRvOTp3HL%2F8Y%3D&st=2020-11-18T16%3A22%3A33Z&se=2020-11-19T00%3A32%3A33Z&sp=r', 'logs/azureml/stdoutlogs.txt': 'https://sa5nmqnmqqzbyzq.blob.core.windows.net/azureml/ExperimentRun/dcid.cab8ad6f-0844-4d4c-998d-e8b812899642/logs/azureml/stdoutlogs.txt?sv=2019-02-02&sr=b&sig=B0aThamOisoKzAzE66HkeYw1nm4tZEAHt9Mnf%2BMN5GU%3D&st=2020-11-18T16%3A22%3A33Z&se=2020-11-19T00%3A32%3A33Z&sp=r'}}\n",
      "\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'Finished'"
      ]
     },
     "metadata": {},
     "execution_count": 148
    }
   ],
   "source": [
    "#Create pipeline, execute pipeline, and wait for response\n",
    "pipeline = Pipeline(workspace=ws, steps=aggregateDataStep)\n",
    "\n",
    "run = experiment.submit(pipeline, pipeline_parameters={\"target_path\": \"source_data_{}\".format(timestamp)})\n",
    "\n",
    "run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Publish pipeline to endpoint\n",
    "published_pipeline = pipeline.publish(name = 'many_models_data_prep',\n",
    "                                     description = 'Gathers and organizes data for many models training job',\n",
    "                                     version = '1',\n",
    "                                     continue_on_step_failure = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sample remote execution\n",
    "#Pipeline execution via REST endpoint requires AAD Token (obtained here from service principal)\n",
    "#Relevant docs:\n",
    "#https://docs.microsoft.com/en-us/azure/machine-learning/how-to-deploy-pipelines\n",
    "#https://github.com/Azure/MachineLearningNotebooks/blob/master/how-to-use-azureml/manage-azureml-service/authentication-in-azureml/authentication-in-azureml.ipynb\n",
    "\n",
    "import requests\n",
    "import os\n",
    "from azureml.core.authentication import ServicePrincipalAuthentication\n",
    "\n",
    "#Service principal creds stored as environment vars\n",
    "client_id = os.environ.get('client_id')\n",
    "tenant_id = os.environ.get('tenant_id')\n",
    "service_principal_password = os.environ.get('service_principal_password')\n",
    "\n",
    "#Leverage ADAL library for obtaining token\n",
    "from adal import AuthenticationContext\n",
    "\n",
    "client_id = client_id\n",
    "client_secret = service_principal_password\n",
    "resource_url = \"https://login.microsoftonline.com\"\n",
    "tenant_id = tenant_id\n",
    "authority = \"{}/{}\".format(resource_url, tenant_id)\n",
    "\n",
    "auth_context = AuthenticationContext(authority)\n",
    "token_response = auth_context.acquire_token_with_client_credentials(\"https://management.azure.com/\", client_id, client_secret)\n",
    "\n",
    "#Format token response for API request to pipeline\n",
    "headers = {'Authorization': 'Bearer {}'.format(token_response['accessToken'])}\n",
    "\n",
    "#Trigger remote pipeline run\n",
    "#Pipeline endpoint can be obtained from AML portal as well\n",
    "response = requests.post(published_pipeline.endpoint,\n",
    "                         headers=headers,\n",
    "                         json={\"ExperimentName\": \"REST_Pipeline_Trigger_Test\",\n",
    "                               \"ParameterAssignments\": {\"target_path\": \"source_data_{}\".format(timestamp)}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Sample code for pulling and partitioning Titanic dataset\n",
    "\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "from csv import reader\n",
    "\n",
    "r = requests.get('https://dprepdata.blob.core.windows.net/demo/Titanic.csv')\n",
    "rows = r.text.split('\\r\\n')\n",
    "formatted_rows = []\n",
    "for row in rows:\n",
    "    read = reader([row], skipinitialspace=True)\n",
    "    vals = [x for x in read]\n",
    "    formatted_rows.append(vals[0])\n",
    "df = pd.DataFrame(formatted_rows[1:], columns=formatted_rows[0])\n",
    "df_s = df[df['Embarked']=='S']\n",
    "df_c = df[df['Embarked']=='C']\n",
    "df_q = df[df['Embarked']=='Q']\n",
    "df_q\n"
   ]
  }
 ]
}