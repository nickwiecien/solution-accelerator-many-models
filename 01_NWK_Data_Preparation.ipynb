{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.7 64-bit ('env': venv)",
   "metadata": {
    "interpreter": {
     "hash": "95f9629f92a95742dc82007aa5e69ca3b1b3071947132bf3b43a4c7c35cf64f4"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set vars for connecting to AML workspace\n",
    "import os\n",
    "\n",
    "subscription_id = os.getenv(\"SUBSCRIPTION_ID\", default=\"\")\n",
    "resource_group = os.getenv(\"RESOURCE_GROUP\", default=\"\")\n",
    "workspace_name = os.getenv(\"WORKSPACE_NAME\", default=\"\")\n",
    "workspace_region = os.getenv(\"WORKSPACE_REGION\", default=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create AML workspace connection\n",
    "from azureml.core import Workspace\n",
    "\n",
    "try:\n",
    "    ws = Workspace(subscription_id=subscription_id, \n",
    "                   resource_group=resource_group, \n",
    "                   workspace_name=workspace_name)\n",
    "    print(\"Workspace configuration succeeded. Skip the workspace creation steps below\")\n",
    "except:\n",
    "    print(\"Workspace does not exist. Creating workspace\")\n",
    "    ws = Workspace.create(name=workspace_name, subscription_id=subscription_id, resource_group=resource_group,\n",
    "                            location=workspace_region, create_resource_group=True, sku='enterprise', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print AML workspace details and write config file\n",
    "print(ws.get_details())\n",
    "\n",
    "ws.write_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a name for your CPU cluster\n",
    "cpu_cluster_name = 'cpucluster'\n",
    "\n",
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "# Verify that cluster does not exist already\n",
    "try:\n",
    "    cpu_cluster = ComputeTarget(workspace=ws, name=cpu_cluster_name)\n",
    "    print('Found an existing cluster, using it instead.')\n",
    "except ComputeTargetException:\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_D13_V2',\n",
    "                                                           min_nodes=0,\n",
    "                                                           max_nodes=10)\n",
    "    cpu_cluster = ComputeTarget.create(ws, cpu_cluster_name, compute_config)\n",
    "    cpu_cluster.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create new experiment for pipeline submission\n",
    "from azureml.core import Experiment\n",
    "\n",
    "experiment = Experiment(ws, 'sample-dataprep-pipeline')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Specify folder which will contain data preparation scripts\n",
    "scripts_folder = './dataprep/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write script to be run in pipeline. Should gather data, upload to AML associated datastore,\n",
    "#create and register datasets\n",
    "%%writefile $scripts_folder/gather_data.py\n",
    "import pandas as pd\n",
    "import requests\n",
    "from csv import reader\n",
    "import os\n",
    "import argparse\n",
    "import json\n",
    "from azureml.core import Run, Workspace, Datastore, Dataset, Experiment\n",
    "\n",
    "parser = argparse.ArgumentParser(\"Aggregate Data\")\n",
    "\n",
    "parser.add_argument(\"--target_path\", type=str, help=\"Target path for data upload\")\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "#Get current run\n",
    "current_run = Run.get_context()\n",
    "#Get associated AML workspace\n",
    "ws = current_run.experiment.workspace\n",
    "#Get default datastore - used to upload data\n",
    "datastore = ws.get_default_datastore()\n",
    "\n",
    "#Target path is passed as a variable argument (can be timestamped)\n",
    "target_path = args.target_path\n",
    "\n",
    "#Pull sample dataset and add to pandas dataframe\n",
    "r = requests.get('https://dprepdata.blob.core.windows.net/demo/Titanic.csv')\n",
    "rows = r.text.split('\\r\\n')\n",
    "formatted_rows = []\n",
    "for row in rows:\n",
    "    read = reader([row], skipinitialspace=True)\n",
    "    vals = [x for x in read]\n",
    "    formatted_rows.append(vals[0])\n",
    "df = pd.DataFrame(formatted_rows[1:], columns=formatted_rows[0])\n",
    "\n",
    "#Partition source dataframe based on values in 'Embarked' column\n",
    "df_s = df[df['Embarked']=='S']\n",
    "df_c = df[df['Embarked']=='C']\n",
    "df_q = df[df['Embarked']=='Q']\n",
    "\n",
    "#Write partitioned dataframes to files in processed data\n",
    "df_s.to_csv('./processed/sourcedata_s.csv', index=False)\n",
    "df_c.to_csv('./processed/sourcedata_c.csv', index=False)\n",
    "df_q.to_csv('./processed/sourcedata_q.csv', index=False)\n",
    "\n",
    "#Upload processed directory to default datastore\n",
    "datastore.upload(src_dir='./processed', target_path=target_path, overwrite=True)\n",
    "\n",
    "from azureml.core.dataset import Dataset\n",
    "\n",
    "# Create file datasets\n",
    "ds_train = Dataset.File.from_files(path=datastore.path(target_path), validate=False)\n",
    "\n",
    "# Register the file datasets\n",
    "dataset_name = 'etf_data'\n",
    "train_dataset_name = dataset_name + '_train'\n",
    "ds_train.register(ws, train_dataset_name, create_new_version=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create timestamp\n",
    "#Note: all files are uploaded to timestamped subdir in AML datastore\n",
    "import time\n",
    "\n",
    "secondsSinceEpoch = time.time()\n",
    "timeObj = time.localtime(secondsSinceEpoch)\n",
    "\n",
    "timestamp = ('%d%d%d%d%d%d' % (\n",
    "timeObj.tm_year, timeObj.tm_mon, timeObj.tm_mday, timeObj.tm_hour, timeObj.tm_min, timeObj.tm_sec))\n",
    "\n",
    "print(timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create PythonScriptStep and associated run configuration\n",
    "from azureml.core.runconfig import RunConfiguration\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "from azureml.core.runconfig import DEFAULT_CPU_IMAGE\n",
    "from azureml.pipeline.core import Pipeline\n",
    "from azureml.pipeline.steps import PythonScriptStep\n",
    "from azureml.pipeline.core import PipelineParameter\n",
    "\n",
    "# create a new runconfig object\n",
    "run_config = RunConfiguration()\n",
    "\n",
    "# enable Docker \n",
    "run_config.environment.docker.enabled = True\n",
    "\n",
    "# set Docker base image to the default CPU-based image\n",
    "run_config.environment.docker.base_image = DEFAULT_CPU_IMAGE\n",
    "\n",
    "# use conda_dependencies.yml to create a conda environment in the Docker image for execution\n",
    "run_config.environment.python.user_managed_dependencies = False\n",
    "\n",
    "# specify CondaDependencies obj\n",
    "run_config.environment.python.conda_dependencies = CondaDependencies.create(conda_packages=['requests', 'pandas'])\n",
    "\n",
    "#Create pipeline parameter\n",
    "pipeline_param = PipelineParameter(name=\"target_path\", default_value=\"source_data_0000\")\n",
    "\n",
    "#aml-pipelines-with-data-dependency-steps.ipynb\n",
    "aggregateDataStep = PythonScriptStep(\n",
    "    script_name=\"gather_data.py\", \n",
    "    arguments=[\"--target_path\", pipeline_param],\n",
    "    compute_target=cpu_cluster, \n",
    "    source_directory=scripts_folder,\n",
    "    runconfig=run_config\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Create pipeline, execute pipeline, and wait for response\n",
    "pipeline = Pipeline(workspace=ws, steps=aggregateDataStep)\n",
    "\n",
    "run = experiment.submit(pipeline, pipeline_parameters={\"target_path\": \"source_data_{}\".format(timestamp)})\n",
    "\n",
    "run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Publish pipeline to endpoint\n",
    "published_pipeline = pipeline.publish(name = 'many_models_data_prep',\n",
    "                                     description = 'Gathers and organizes data for many models training job',\n",
    "                                     version = '1',\n",
    "                                     continue_on_step_failure = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sample remote execution\n",
    "#Pipeline execution via REST endpoint requires AAD Token (obtained here from service principal)\n",
    "#Relevant docs:\n",
    "#https://docs.microsoft.com/en-us/azure/machine-learning/how-to-deploy-pipelines\n",
    "#https://github.com/Azure/MachineLearningNotebooks/blob/master/how-to-use-azureml/manage-azureml-service/authentication-in-azureml/authentication-in-azureml.ipynb\n",
    "\n",
    "import requests\n",
    "import os\n",
    "from azureml.core.authentication import ServicePrincipalAuthentication\n",
    "\n",
    "#Service principal creds stored as environment vars\n",
    "client_id = os.environ.get('client_id')\n",
    "tenant_id = os.environ.get('tenant_id')\n",
    "service_principal_password = os.environ.get('service_principal_password')\n",
    "\n",
    "#Leverage ADAL library for obtaining token\n",
    "from adal import AuthenticationContext\n",
    "\n",
    "client_id = client_id\n",
    "client_secret = service_principal_password\n",
    "resource_url = \"https://login.microsoftonline.com\"\n",
    "tenant_id = tenant_id\n",
    "authority = \"{}/{}\".format(resource_url, tenant_id)\n",
    "\n",
    "auth_context = AuthenticationContext(authority)\n",
    "token_response = auth_context.acquire_token_with_client_credentials(\"https://management.azure.com/\", client_id, client_secret)\n",
    "\n",
    "#Format token response for API request to pipeline\n",
    "headers = {'Authorization': 'Bearer {}'.format(token_response['accessToken'])}\n",
    "\n",
    "#Trigger remote pipeline run\n",
    "#Pipeline endpoint can be obtained from AML portal as well\n",
    "response = requests.post(published_pipeline.endpoint,\n",
    "                         headers=headers,\n",
    "                         json={\"ExperimentName\": \"REST_Pipeline_Trigger_Test\",\n",
    "                               \"ParameterAssignments\": {\"target_path\": \"source_data_{}\".format(timestamp)}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Sample code for pulling and partitioning Titanic dataset\n",
    "\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "from csv import reader\n",
    "\n",
    "r = requests.get('https://dprepdata.blob.core.windows.net/demo/Titanic.csv')\n",
    "rows = r.text.split('\\r\\n')\n",
    "formatted_rows = []\n",
    "for row in rows:\n",
    "    read = reader([row], skipinitialspace=True)\n",
    "    vals = [x for x in read]\n",
    "    formatted_rows.append(vals[0])\n",
    "df = pd.DataFrame(formatted_rows[1:], columns=formatted_rows[0])\n",
    "df_s = df[df['Embarked']=='S']\n",
    "df_c = df[df['Embarked']=='C']\n",
    "df_q = df[df['Embarked']=='Q']\n",
    "df_q\n"
   ]
  }
 ]
}