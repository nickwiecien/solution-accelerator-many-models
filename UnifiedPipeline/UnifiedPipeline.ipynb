{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.7 64-bit ('env': venv)",
   "metadata": {
    "interpreter": {
     "hash": "95f9629f92a95742dc82007aa5e69ca3b1b3071947132bf3b43a4c7c35cf64f4"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Connect to AML Workspace\n",
    "from azureml.core import Workspace\n",
    "subscription_id = os.getenv(\"SUBSCRIPTION_ID\", default=\"\")\n",
    "resource_group = os.getenv(\"RESOURCE_GROUP\", default=\"\")\n",
    "workspace_name = os.getenv(\"WORKSPACE_NAME\", default=\"\")\n",
    "workspace_region = os.getenv(\"WORKSPACE_REGION\", default=\"\")\n",
    "\n",
    "try:\n",
    "    ws = Workspace.from_config()\n",
    "    # ws = Workspace(subscription_id=subscription_id, \n",
    "    #                resource_group=resource_group, \n",
    "    #                workspace_name=workspace_name)\n",
    "    print(\"Workspace configuration succeeded. Skip the workspace creation steps below\")\n",
    "except:\n",
    "    print(\"Workspace does not exist. Creating workspace\")\n",
    "    ws = Workspace.create(name=workspace_name, subscription_id=subscription_id, resource_group=resource_group,\n",
    "                            location=workspace_region, create_resource_group=True, sku='enterprise', exist_ok=True)\n",
    "\n",
    "#Select AML Compute Cluster\n",
    "cpu_cluster_name = 'cpucluster'\n",
    "\n",
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "# Verify that cluster does not exist already\n",
    "try:\n",
    "    cpu_cluster = ComputeTarget(workspace=ws, name=cpu_cluster_name)\n",
    "    print('Found an existing cluster, using it instead.')\n",
    "except ComputeTargetException:\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_D13_V2',\n",
    "                                                           min_nodes=0,\n",
    "                                                           max_nodes=10)\n",
    "    cpu_cluster = ComputeTarget.create(ws, cpu_cluster_name, compute_config)\n",
    "    cpu_cluster.wait_for_completion(show_output=True)\n",
    "\n",
    "#Create Experiment\n",
    "from azureml.core import Experiment\n",
    "experiment = Experiment(ws, 'unified-dataprep-pipeline')\n",
    "\n",
    "#Get Default AML Datastore\n",
    "from azureml.core import Datastore\n",
    "ds = ws.get_default_datastore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set Up Data Prep Pipeline Step\n",
    "from azureml.core.runconfig import RunConfiguration\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "from azureml.core.runconfig import DEFAULT_CPU_IMAGE\n",
    "from azureml.pipeline.core import Pipeline\n",
    "from azureml.pipeline.steps import PythonScriptStep\n",
    "from azureml.pipeline.core import PipelineParameter, PipelineData\n",
    "from azureml.data import OutputFileDatasetConfig\n",
    "\n",
    "# create a new runconfig object\n",
    "dataprep_run_config = RunConfiguration()\n",
    "\n",
    "# enable Docker \n",
    "dataprep_run_config.environment.docker.enabled = True\n",
    "\n",
    "# set Docker base image to the default CPU-based image\n",
    "dataprep_run_config.environment.docker.base_image = DEFAULT_CPU_IMAGE\n",
    "\n",
    "# use conda_dependencies.yml to create a conda environment in the Docker image for execution\n",
    "dataprep_run_config.environment.python.user_managed_dependencies = False\n",
    "\n",
    "# specify CondaDependencies obj\n",
    "dataprep_run_config.environment.python.conda_dependencies = CondaDependencies.create(conda_packages=['requests', 'pandas'])\n",
    "dataprep_run_config.environment.python.conda_dependencies.add_pip_package('snowflake-connector-python[pandas]')\n",
    "dataprep_run_config.environment.python.conda_dependencies.add_pip_package('azureml-opendatasets')\n",
    "\n",
    "#Configure three output datasets to be rgistered in the workspace (source data, training data, and inferencing data)\n",
    "inference_dataset = OutputFileDatasetConfig(name='oj_inference_data', destination=(ds, 'oj_inference_data/{run-id}')).register_on_complete(name='oj_inference_data')\n",
    "train_dataset = OutputFileDatasetConfig(name='oj_train_data',destination=(ds, 'oj_train_data/{run-id}')).register_on_complete(name='oj_train_data')\n",
    "data_dataset = OutputFileDatasetConfig(name='oj_sales_data', destination=(ds, 'oj_sales_data/{run-id}')).register_on_complete(name='oj_sales_data')\n",
    "\n",
    "#Alternate Dataset configuration - omits experimental classes\n",
    "# inference_data = PipelineData('oj_inference_data', datastore = ds, is_directory=True)\n",
    "# inference_dataset = inference_data.as_dataset()\n",
    "# inference_dataset.register(name='oj_inference_data', create_new_version=True)\n",
    "\n",
    "# train_data = PipelineData('oj_train_data', datastore = ds, is_directory=True)\n",
    "# train_dataset = train_data.as_dataset()\n",
    "# train_dataset.register(name='oj_train_data', create_new_version=True)\n",
    "\n",
    "# data_path = PipelineData('oj_sales_data', datastore=ds, is_directory=True)\n",
    "# data_dataset = data_path.as_dataset()\n",
    "# data_dataset.register(name='oj_sales_data', create_new_version=True)\n",
    "\n",
    "#Create PythonScriptStep to gather data from remote source and register as AML dataset\n",
    "aggregate_data_step = PythonScriptStep(\n",
    "    script_name=\"gather_data.py\", \n",
    "    arguments=[\"--train_path\", train_dataset, \"--inference_path\", inference_dataset, \"--data_path\", data_dataset],\n",
    "    outputs=[data_dataset, train_dataset, inference_dataset],\n",
    "    compute_target=cpu_cluster, \n",
    "    source_directory='./dataprep',\n",
    "    allow_reuse=False,\n",
    "    runconfig=dataprep_run_config\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AutoML Training Step\n",
    "import logging\n",
    "from automl_train.scripts.helper import write_automl_settings_to_file, build_parallel_run_config\n",
    "from azureml.pipeline.core import PipelineParameter, PipelineData\n",
    "\n",
    "#Set up AutoML configuration and write to a file\n",
    "automl_settings = {\n",
    "    \"task\" : 'forecasting',\n",
    "    \"primary_metric\" : 'normalized_root_mean_squared_error',\n",
    "    \"iteration_timeout_minutes\" : 10, # This needs to be changed based on the dataset. We ask customer to explore how long training is taking before settings this value\n",
    "    \"iterations\" : 15,\n",
    "    \"experiment_timeout_hours\" : 1,\n",
    "    \"label_column_name\" : 'Quantity',\n",
    "    \"n_cross_validations\" : 3,\n",
    "    \"verbosity\" : logging.INFO, \n",
    "    \"debug_log\": 'automl_oj_sales_debug.txt',\n",
    "    \"time_column_name\": 'WeekStarting',\n",
    "    \"max_horizon\" : 20,\n",
    "    \"track_child_runs\": False,\n",
    "    \"group_column_names\": ['Store', 'Brand'],\n",
    "    \"grain_column_names\": ['Store', 'Brand']\n",
    "}\n",
    "write_automl_settings_to_file(automl_settings)\n",
    "\n",
    "#Set up training environment (reused for inferencing later)\n",
    "from automl_train.scripts.helper import get_automl_environment\n",
    "train_env = get_automl_environment(workspace=ws, automl_settings_dict=automl_settings)\n",
    "\n",
    "#Configure your cluster\n",
    "node_count=4\n",
    "process_count_per_node=8\n",
    "run_invocation_timeout=3700\n",
    "\n",
    "#Build parallel run step configuration\n",
    "parallel_run_config = build_parallel_run_config(train_env, cpu_cluster, node_count, process_count_per_node, run_invocation_timeout)\n",
    "training_output_name = \"training_output\"\n",
    "train_output_dir = PipelineData(name=training_output_name, \n",
    "                          datastore=ds)\n",
    "\n",
    "from azureml.pipeline.steps import ParallelRunStep\n",
    "\n",
    "#Define training ParallelRunStep\n",
    "train_parallel_run_step = ParallelRunStep(\n",
    "    name=\"many-models-training-automl\",\n",
    "    parallel_run_config=parallel_run_config,\n",
    "    allow_reuse = False,\n",
    "    inputs=[train_dataset.as_input(name='oj_train_data')],\n",
    "    output=train_output_dir,\n",
    ")\n",
    "\n",
    "#Specify that training step much occur after data gathering step\n",
    "train_parallel_run_step.run_after(aggregate_data_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set up custom model training step\n",
    "from azureml.core import Environment\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "from azureml.pipeline.steps import ParallelRunConfig\n",
    "\n",
    "custom_model_train_env = Environment(name=\"many_models_environment\")\n",
    "custom_model_train_conda_deps = CondaDependencies.create(pip_packages=['sklearn', 'pandas', 'joblib', 'azureml-defaults', 'azureml-core', 'azureml-dataprep[fuse]'])\n",
    "custom_model_train_env.python.conda_dependencies = custom_model_train_conda_deps\n",
    "\n",
    "processes_per_node = 8\n",
    "node_count = 1\n",
    "timeout = 180\n",
    "\n",
    "parallel_run_config = ParallelRunConfig(\n",
    "    source_directory='./custom_scripts/scripts',\n",
    "    entry_script='train.py',\n",
    "    mini_batch_size=\"1\",\n",
    "    run_invocation_timeout=timeout,\n",
    "    error_threshold=10,\n",
    "    output_action=\"append_row\",\n",
    "    environment=custom_model_train_env,\n",
    "    process_count_per_node=processes_per_node,\n",
    "    compute_target=cpu_cluster,\n",
    "    node_count=node_count)\n",
    "\n",
    "custom_model_output_dir = PipelineData(name=\"training_output\", datastore=ds)\n",
    "\n",
    "custom_model_training_parallel_run_step = ParallelRunStep(\n",
    "    name=\"many-models-training-custom\",\n",
    "    parallel_run_config=parallel_run_config,\n",
    "    inputs=[train_dataset.as_input(name='oj_train_data')],\n",
    "    output=custom_model_output_dir,\n",
    "    allow_reuse=False,\n",
    "    arguments=['--target_column', 'Quantity', \n",
    "               '--timestamp_column', 'WeekStarting', \n",
    "               '--timeseries_id_columns', 'Store', 'Brand',\n",
    "               '--drop_columns', 'Revenue', 'Store', 'Brand',\n",
    "               '--model_type', 'lr',\n",
    "               '--test_size', 20]\n",
    ")\n",
    "\n",
    "custom_model_training_parallel_run_step.run_after(aggregate_data_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Pipeline Step for Inferencing\n",
    "forecast_env = train_env\n",
    "\n",
    "from automl_inference.scripts.helper import build_parallel_run_config_for_forecasting\n",
    "\n",
    "#Set up configuration for parallel inferencing run\n",
    "node_count=2\n",
    "process_count_per_node=6\n",
    "run_invocation_timeout=300 # this timeout(in seconds), for larger models need to change this to a higher timeout\n",
    "\n",
    "parallel_run_config = build_parallel_run_config_for_forecasting(forecast_env, cpu_cluster, node_count, process_count_per_node, run_invocation_timeout)\n",
    "\n",
    "from azureml.pipeline.core import PipelineData\n",
    "from azureml.pipeline.steps import ParallelRunStep\n",
    "\n",
    "#Define location where forecasting output will be saved\n",
    "forecasting_output_name = 'automl_forecasting_output'\n",
    "forecast_output_dir = PipelineData(name = forecasting_output_name, \n",
    "                          datastore = ds)\n",
    "\n",
    "#Create parallel inferencing step\n",
    "inference_parallel_run_step = ParallelRunStep(\n",
    "    name=\"many-models-forecasting-automl\",\n",
    "    parallel_run_config=parallel_run_config,\n",
    "    inputs=[inference_dataset.as_input(name='oj_inference_data')], \n",
    "    output=forecast_output_dir,\n",
    "    arguments=[\n",
    "              '--group_column_names', 'Store', 'Brand',\n",
    "              '--time_column_name', 'WeekStarting', #[Optional] # this is needed for timeseries\n",
    "              '--target_column_name', 'Quantity', # [Optional] Needs to be passed only if inference data contains target column.\n",
    "              ])\n",
    "\n",
    "#Specify that inferencing must happen after training\n",
    "inference_parallel_run_step.run_after(train_parallel_run_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Environment\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "\n",
    "custom_model_forecast_env = Environment(name=\"many_models_environment\")\n",
    "custom_model_forecast_conda_deps = CondaDependencies.create(pip_packages=['sklearn', 'pandas', 'joblib', 'azureml-defaults', 'numpy', 'azureml-core', 'azureml-dataprep[fuse]'])\n",
    "custom_model_forecast_env.python.conda_dependencies = custom_model_forecast_conda_deps\n",
    "\n",
    "from azureml.pipeline.steps import ParallelRunConfig \n",
    "\n",
    "process_count_per_node = 6\n",
    "node_count = 1\n",
    "timeout = 180\n",
    "\n",
    "parallel_run_config = ParallelRunConfig(\n",
    "    source_directory='./custom_scripts/scripts',\n",
    "    entry_script='forecast.py',\n",
    "    mini_batch_size='1',\n",
    "    run_invocation_timeout=timeout, \n",
    "    error_threshold=10,\n",
    "    output_action='append_row', \n",
    "    environment=custom_model_forecast_env, \n",
    "    process_count_per_node=process_count_per_node, \n",
    "    compute_target=cpu_cluster, \n",
    "    node_count=node_count\n",
    ")\n",
    "\n",
    "custom_model_output_dir = PipelineData(name='custom_forecasting_output', datastore=ds)\n",
    "\n",
    "custom_model_forecast_parallel_run_step = ParallelRunStep(\n",
    "    name=\"many-models-forecasting-custom\",\n",
    "    parallel_run_config=parallel_run_config,\n",
    "    inputs=[inference_dataset.as_input(name='oj_inference_data')],\n",
    "    output=custom_model_output_dir,\n",
    "    allow_reuse=False,\n",
    "    arguments=['--timestamp_column', 'WeekStarting',\n",
    "               '--timeseries_id_columns', 'Store', 'Brand',\n",
    "               '--model_type', 'lr']\n",
    ")\n",
    "\n",
    "custom_model_forecast_parallel_run_step.run_after(custom_model_training_parallel_run_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Move Data Step\n",
    "#Execute python script that formats and moves forecasting results to new location \n",
    "\n",
    "copy_output_name = \"copy_data_outputs\"\n",
    "copy_output_dir = PipelineData(name=copy_output_name, \n",
    "                          datastore=ds)\n",
    "copy_data_step = PythonScriptStep(\n",
    "    script_name=\"move_data.py\", \n",
    "    arguments=[\"--automl_parallel_run_step_output\", forecast_output_dir, \"--custom_model_parallel_run_step_output\", custom_model_output_dir, '--copy_outputs_dir', copy_output_dir],\n",
    "    inputs=[forecast_output_dir, custom_model_output_dir],\n",
    "    outputs=[copy_output_dir],\n",
    "    compute_target=cpu_cluster, \n",
    "    source_directory='./copydata',\n",
    "    allow_reuse=False,\n",
    "    name=\"write-prediction-data\"\n",
    ")\n",
    "\n",
    "#Specify that data copy must happen after inferencing\n",
    "copy_data_step.run_after(inference_parallel_run_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# #Create pipeline, execute pipeline, and wait for response\n",
    "pipeline = Pipeline(workspace=ws, steps=[aggregate_data_step, train_parallel_run_step, custom_model_training_parallel_run_step, custom_model_forecast_parallel_run_step, inference_parallel_run_step, copy_data_step])\n",
    "run = experiment.submit(pipeline)\n",
    "run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Publish pipeline to endpoint\n",
    "published_pipeline = pipeline.publish(name = 'many_models_sample',\n",
    "                                     description = 'Gathers data, trains models, generates forecasts, and stores results',\n",
    "                                     version = '1',\n",
    "                                     continue_on_step_failure = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sample remote execution\n",
    "#Pipeline execution via REST endpoint requires AAD Token (obtained here from service principal)\n",
    "#Relevant docs:\n",
    "#https://docs.microsoft.com/en-us/azure/machine-learning/how-to-deploy-pipelines\n",
    "#https://github.com/Azure/MachineLearningNotebooks/blob/master/how-to-use-azureml/manage-azureml-service/authentication-in-azureml/authentication-in-azureml.ipynb\n",
    "\n",
    "import requests\n",
    "import os\n",
    "from azureml.core.authentication import ServicePrincipalAuthentication\n",
    "\n",
    "#Service principal creds stored as environment vars\n",
    "client_id = os.environ.get('client_id')\n",
    "tenant_id = os.environ.get('tenant_id')\n",
    "service_principal_password = os.environ.get('service_principal_password')\n",
    "pipeline_endpoint = os.environ.get('pipeline_endpoint')\n",
    "\n",
    "#Leverage ADAL library for obtaining token\n",
    "from adal import AuthenticationContext\n",
    "\n",
    "client_id = client_id\n",
    "client_secret = service_principal_password\n",
    "resource_url = \"https://login.microsoftonline.com\"\n",
    "tenant_id = tenant_id\n",
    "authority = \"{}/{}\".format(resource_url, tenant_id)\n",
    "\n",
    "auth_context = AuthenticationContext(authority)\n",
    "token_response = auth_context.acquire_token_with_client_credentials(\"https://management.azure.com/\", client_id, client_secret)\n",
    "\n",
    "#Format token response for API request to pipeline\n",
    "headers = {'Authorization': 'Bearer {}'.format(token_response['accessToken'])}\n",
    "\n",
    "#Trigger remote pipeline run\n",
    "#Pipeline endpoint can be obtained from AML portal as well\n",
    "response = requests.post(pipeline_endpoint,\n",
    "                         headers=headers,\n",
    "                         json={\"ExperimentName\": \"REST_Unified_Pipeline_Trigger_Test\"})"
   ]
  }
 ]
}