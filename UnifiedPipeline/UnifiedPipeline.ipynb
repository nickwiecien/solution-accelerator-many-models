{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.7 64-bit ('env')",
   "metadata": {
    "interpreter": {
     "hash": "9a5315d6b50b87618ecda6cf55419f72e5dc225d1caf95f40d30cde6a318ae2a"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Failure while loading azureml_run_type_providers. Failed to load entrypoint automl = azureml.train.automl.run:AutoMLRun._from_run_dto with exception (pyarrow 2.0.0 (c:\\users\\nikwieci\\documents\\demos\\nwk_manymodels\\many-models-full-pipeline\\env\\lib\\site-packages), Requirement.parse('pyarrow<2.0.0,>=0.17.0'), {'azureml-dataset-runtime'}).\n",
      "Warning: Falling back to use azure cli login credentials.\n",
      "If you run your code in unattended mode, i.e., where you can't give a user input, then we recommend to use ServicePrincipalAuthentication or MsiAuthentication.\n",
      "Please refer to aka.ms/aml-notebook-auth for different authentication mechanisms in azureml-sdk.\n",
      "Workspace configuration succeeded. Skip the workspace creation steps below\n",
      "Found an existing cluster, using it instead.\n"
     ]
    }
   ],
   "source": [
    "#Connect to AML Workspace\n",
    "from azureml.core import Workspace\n",
    "subscription_id = os.getenv(\"SUBSCRIPTION_ID\", default=\"f3e38aaa-dd9c-4f17-95c1-ef3ff472da61\")\n",
    "resource_group = os.getenv(\"RESOURCE_GROUP\", default=\"nwk-aml-nov20-rg\")\n",
    "workspace_name = os.getenv(\"WORKSPACE_NAME\", default=\"nwk-aml-nov20\")\n",
    "workspace_region = os.getenv(\"WORKSPACE_REGION\", default=\"eastus2\")\n",
    "\n",
    "try:\n",
    "    ws = Workspace(subscription_id=subscription_id, \n",
    "                   resource_group=resource_group, \n",
    "                   workspace_name=workspace_name)\n",
    "    print(\"Workspace configuration succeeded. Skip the workspace creation steps below\")\n",
    "except:\n",
    "    print(\"Workspace does not exist. Creating workspace\")\n",
    "    ws = Workspace.create(name=workspace_name, subscription_id=subscription_id, resource_group=resource_group,\n",
    "                            location=workspace_region, create_resource_group=True, sku='enterprise', exist_ok=True)\n",
    "\n",
    "#Select AML Compute Cluster\n",
    "cpu_cluster_name = 'cpucluster'\n",
    "\n",
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "# Verify that cluster does not exist already\n",
    "try:\n",
    "    cpu_cluster = ComputeTarget(workspace=ws, name=cpu_cluster_name)\n",
    "    print('Found an existing cluster, using it instead.')\n",
    "except ComputeTargetException:\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_D13_V2',\n",
    "                                                           min_nodes=0,\n",
    "                                                           max_nodes=10)\n",
    "    cpu_cluster = ComputeTarget.create(ws, cpu_cluster_name, compute_config)\n",
    "    cpu_cluster.wait_for_completion(show_output=True)\n",
    "\n",
    "#Create Experiment\n",
    "from azureml.core import Experiment\n",
    "experiment = Experiment(ws, 'unified-dataprep-pipeline')\n",
    "\n",
    "#Get Default AML Datastore\n",
    "from azureml.core import Datastore\n",
    "ds = ws.get_default_datastore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Class OutputFileDatasetConfig: This is an experimental class, and may change at any time.<br/>For more information, see https://aka.ms/azuremlexperimental.\n",
      "Class OutputDatasetConfig: This is an experimental class, and may change at any time.<br/>For more information, see https://aka.ms/azuremlexperimental.\n",
      "Class RegistrationConfiguration: This is an experimental class, and may change at any time.<br/>For more information, see https://aka.ms/azuremlexperimental.\n",
      "Class OutputFileDatasetConfig: This is an experimental class, and may change at any time.<br/>For more information, see https://aka.ms/azuremlexperimental.\n",
      "Class OutputDatasetConfig: This is an experimental class, and may change at any time.<br/>For more information, see https://aka.ms/azuremlexperimental.\n",
      "Class RegistrationConfiguration: This is an experimental class, and may change at any time.<br/>For more information, see https://aka.ms/azuremlexperimental.\n",
      "Class OutputFileDatasetConfig: This is an experimental class, and may change at any time.<br/>For more information, see https://aka.ms/azuremlexperimental.\n",
      "Class OutputDatasetConfig: This is an experimental class, and may change at any time.<br/>For more information, see https://aka.ms/azuremlexperimental.\n",
      "Class RegistrationConfiguration: This is an experimental class, and may change at any time.<br/>For more information, see https://aka.ms/azuremlexperimental.\n"
     ]
    }
   ],
   "source": [
    "#Set Up Data Prep Pipeline Step\n",
    "from azureml.core.runconfig import RunConfiguration\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "from azureml.core.runconfig import DEFAULT_CPU_IMAGE\n",
    "from azureml.pipeline.core import Pipeline\n",
    "from azureml.pipeline.steps import PythonScriptStep\n",
    "from azureml.pipeline.core import PipelineParameter, PipelineData\n",
    "from azureml.data import OutputFileDatasetConfig\n",
    "\n",
    "# create a new runconfig object\n",
    "dataprep_run_config = RunConfiguration()\n",
    "\n",
    "# enable Docker \n",
    "dataprep_run_config.environment.docker.enabled = True\n",
    "\n",
    "# set Docker base image to the default CPU-based image\n",
    "dataprep_run_config.environment.docker.base_image = DEFAULT_CPU_IMAGE\n",
    "\n",
    "# use conda_dependencies.yml to create a conda environment in the Docker image for execution\n",
    "dataprep_run_config.environment.python.user_managed_dependencies = False\n",
    "\n",
    "# specify CondaDependencies obj\n",
    "dataprep_run_config.environment.python.conda_dependencies = CondaDependencies.create(conda_packages=['requests', 'pandas'])\n",
    "dataprep_run_config.environment.python.conda_dependencies.add_pip_package('snowflake-connector-python[pandas]')\n",
    "dataprep_run_config.environment.python.conda_dependencies.add_pip_package('azureml-opendatasets')\n",
    "\n",
    "#Configure three output datasets to be rgistered in the workspace (source data, training data, and inferencing data)\n",
    "inference_dataset = OutputFileDatasetConfig(name='oj_inference_data', destination=(ds, 'oj_inference_data/{run-id}')).register_on_complete(name='oj_inference_data')\n",
    "train_dataset = OutputFileDatasetConfig(name='oj_train_data',destination=(ds, 'oj_train_data/{run-id}')).register_on_complete(name='oj_train_data')\n",
    "data_dataset = OutputFileDatasetConfig(name='oj_sales_data', destination=(ds, 'oj_sales_data/{run-id}')).register_on_complete(name='oj_sales_data')\n",
    "\n",
    "#Alternate Dataset configuration - omits experimental classes\n",
    "# inference_data = PipelineData('oj_inference_data', datastore = ds, is_directory=True)\n",
    "# inference_dataset = inference_data.as_dataset()\n",
    "# inference_dataset.register(name='oj_inference_data', create_new_version=True)\n",
    "\n",
    "# train_data = PipelineData('oj_train_data', datastore = ds, is_directory=True)\n",
    "# train_dataset = train_data.as_dataset()\n",
    "# train_dataset.register(name='oj_train_data', create_new_version=True)\n",
    "\n",
    "# data_path = PipelineData('oj_sales_data', datastore=ds, is_directory=True)\n",
    "# data_dataset = data_path.as_dataset()\n",
    "# data_dataset.register(name='oj_sales_data', create_new_version=True)\n",
    "\n",
    "#Create PythonScriptStep to gather data from remote source and register as AML dataset\n",
    "aggregate_data_step = PythonScriptStep(\n",
    "    script_name=\"gather_data.py\", \n",
    "    arguments=[\"--train_path\", train_dataset, \"--inference_path\", inference_dataset, \"--data_path\", data_dataset],\n",
    "    outputs=[data_dataset, train_dataset, inference_dataset],\n",
    "    compute_target=cpu_cluster, \n",
    "    source_directory='./dataprep',\n",
    "    allow_reuse=False,\n",
    "    runconfig=dataprep_run_config\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "WARNING:root:Received unrecognized parameter task\n",
      "WARNING:root:Received unrecognized parameter experiment_timeout_hours\n",
      "WARNING:root:Received unrecognized parameter time_column_name\n",
      "WARNING:root:Received unrecognized parameter max_horizon\n",
      "WARNING:root:Received unrecognized parameter group_column_names\n",
      "WARNING:root:Received unrecognized parameter grain_column_names\n"
     ]
    }
   ],
   "source": [
    "#AutoML Training Step\n",
    "import logging\n",
    "from automl_train.scripts.helper import write_automl_settings_to_file, build_parallel_run_config\n",
    "from azureml.pipeline.core import PipelineParameter, PipelineData\n",
    "\n",
    "#Set up AutoML configuration and write to a file\n",
    "automl_settings = {\n",
    "    \"task\" : 'forecasting',\n",
    "    \"primary_metric\" : 'normalized_root_mean_squared_error',\n",
    "    \"iteration_timeout_minutes\" : 10, # This needs to be changed based on the dataset. We ask customer to explore how long training is taking before settings this value\n",
    "    \"iterations\" : 15,\n",
    "    \"experiment_timeout_hours\" : 1,\n",
    "    \"label_column_name\" : 'Quantity',\n",
    "    \"n_cross_validations\" : 3,\n",
    "    \"verbosity\" : logging.INFO, \n",
    "    \"debug_log\": 'automl_oj_sales_debug.txt',\n",
    "    \"time_column_name\": 'WeekStarting',\n",
    "    \"max_horizon\" : 20,\n",
    "    \"track_child_runs\": True,\n",
    "    \"group_column_names\": ['Store', 'Brand'],\n",
    "    \"grain_column_names\": ['Store', 'Brand']\n",
    "}\n",
    "write_automl_settings_to_file(automl_settings)\n",
    "\n",
    "#Set up training environment (reused for inferencing later)\n",
    "from automl_train.scripts.helper import get_automl_environment\n",
    "train_env = get_automl_environment(workspace=ws, automl_settings_dict=automl_settings)\n",
    "\n",
    "#Configure your cluster\n",
    "node_count=4\n",
    "process_count_per_node=8\n",
    "run_invocation_timeout=3700\n",
    "\n",
    "#Build parallel run step configuration\n",
    "parallel_run_config = build_parallel_run_config(train_env, cpu_cluster, node_count, process_count_per_node, run_invocation_timeout)\n",
    "training_output_name = \"training_output\"\n",
    "train_output_dir = PipelineData(name=training_output_name, \n",
    "                          datastore=ds)\n",
    "\n",
    "from azureml.pipeline.steps import ParallelRunStep\n",
    "\n",
    "#Define training ParallelRunStep\n",
    "train_parallel_run_step = ParallelRunStep(\n",
    "    name=\"many-models-training-automl\",\n",
    "    parallel_run_config=parallel_run_config,\n",
    "    allow_reuse = False,\n",
    "    inputs=[train_dataset.as_input(name='oj_train_data')],\n",
    "    output=train_output_dir,\n",
    ")\n",
    "\n",
    "#Specify that training step much occur after data gathering step\n",
    "train_parallel_run_step.run_after(aggregate_data_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Pipeline Step for Inferencing\n",
    "forecast_env = train_env\n",
    "\n",
    "from automl_inference.scripts.helper import build_parallel_run_config_for_forecasting\n",
    "\n",
    "#Set up configuration for parallel inferencing run\n",
    "node_count=2\n",
    "process_count_per_node=6\n",
    "run_invocation_timeout=300 # this timeout(in seconds), for larger models need to change this to a higher timeout\n",
    "\n",
    "parallel_run_config = build_parallel_run_config_for_forecasting(forecast_env, cpu_cluster, node_count, process_count_per_node, run_invocation_timeout)\n",
    "\n",
    "from azureml.pipeline.core import PipelineData\n",
    "from azureml.pipeline.steps import ParallelRunStep\n",
    "\n",
    "#Define location where forecasting output will be saved\n",
    "forecasting_output_name = 'forecasting_output'\n",
    "forecast_output_dir = PipelineData(name = forecasting_output_name, \n",
    "                          datastore = ds)\n",
    "\n",
    "#Create parallel inferencing step\n",
    "inference_parallel_run_step = ParallelRunStep(\n",
    "    name=\"many-models-forecasting-automl\",\n",
    "    parallel_run_config=parallel_run_config,\n",
    "    inputs=[inference_dataset.as_input(name='oj_inference_data')], \n",
    "    output=forecast_output_dir,\n",
    "    arguments=[\n",
    "              '--group_column_names', 'Store', 'Brand',\n",
    "              '--time_column_name', 'WeekStarting', #[Optional] # this is needed for timeseries\n",
    "              '--target_column_name', 'Quantity', # [Optional] Needs to be passed only if inference data contains target column.\n",
    "              ])\n",
    "\n",
    "#Specify that inferencing must happen after training\n",
    "inference_parallel_run_step.run_after(train_parallel_run_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Move Data Step\n",
    "#Execute python script that formats and moves forecasting results to new location \n",
    "copy_data_step = PythonScriptStep(\n",
    "    script_name=\"move_data.py\", \n",
    "    arguments=[\"--parallel_run_step_output\", forecast_output_dir],\n",
    "    inputs=[forecast_output_dir],\n",
    "    compute_target=cpu_cluster, \n",
    "    source_directory='./copydata',\n",
    "    allow_reuse=False,\n",
    "    name=\"write-prediction-data\"\n",
    ")\n",
    "\n",
    "#Specify that data copy must happen after inferencing\n",
    "copy_data_step.run_after(inference_parallel_run_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "143-a736-f568e0554edd/logs/azureml/dataprep/python_span_371b7550-d572-41dc-9779-41f70169f669.jsonl?sv=2019-02-02&sr=b&sig=JPF3il%2B7BWyn1%2FZYEkPTbXxq5MB2puvQ%2FJhEr1Vfu5o%3D&st=2020-12-05T07%3A14%3A46Z&se=2020-12-05T15%3A24%3A46Z&sp=r', 'logs/azureml/dataprep/python_span_589e5738-ddd6-4d0f-8429-5af6fa9ef95a.jsonl': 'https://sa5nmqnmqqzbyzq.blob.core.windows.net/azureml/ExperimentRun/dcid.0c224c09-ef50-4143-a736-f568e0554edd/logs/azureml/dataprep/python_span_589e5738-ddd6-4d0f-8429-5af6fa9ef95a.jsonl?sv=2019-02-02&sr=b&sig=utTR92fXuEq6r0k%2Ffuiy8t8%2FIVejuU7F%2BBUYJB8ZREI%3D&st=2020-12-05T07%3A14%3A46Z&se=2020-12-05T15%3A24%3A46Z&sp=r', 'logs/azureml/dataprep/python_span_6722b6e9-4264-4819-a8be-a567cb0852d4.jsonl': 'https://sa5nmqnmqqzbyzq.blob.core.windows.net/azureml/ExperimentRun/dcid.0c224c09-ef50-4143-a736-f568e0554edd/logs/azureml/dataprep/python_span_6722b6e9-4264-4819-a8be-a567cb0852d4.jsonl?sv=2019-02-02&sr=b&sig=fijhj9Y4w5lih2yPVHC3N5wiJDscOBWejGdeQi1Yy%2BY%3D&st=2020-12-05T07%3A14%3A46Z&se=2020-12-05T15%3A24%3A46Z&sp=r', 'logs/azureml/dataprep/python_span_6b453ab8-b282-41de-9ad4-0f73b1aa3b55.jsonl': 'https://sa5nmqnmqqzbyzq.blob.core.windows.net/azureml/ExperimentRun/dcid.0c224c09-ef50-4143-a736-f568e0554edd/logs/azureml/dataprep/python_span_6b453ab8-b282-41de-9ad4-0f73b1aa3b55.jsonl?sv=2019-02-02&sr=b&sig=0t3gNQF5YMVUilV7USp7W%2F8Jib07Sn2mIvCf8kkTzK0%3D&st=2020-12-05T07%3A14%3A46Z&se=2020-12-05T15%3A24%3A46Z&sp=r', 'logs/azureml/dataprep/python_span_70bc508a-23ab-48e2-a626-1516d732ecd7.jsonl': 'https://sa5nmqnmqqzbyzq.blob.core.windows.net/azureml/ExperimentRun/dcid.0c224c09-ef50-4143-a736-f568e0554edd/logs/azureml/dataprep/python_span_70bc508a-23ab-48e2-a626-1516d732ecd7.jsonl?sv=2019-02-02&sr=b&sig=umie1ZderzywNT7O8K8eqR2lVXWwjYyd2nWbFnwmReQ%3D&st=2020-12-05T07%3A14%3A46Z&se=2020-12-05T15%3A24%3A46Z&sp=r', 'logs/azureml/dataprep/python_span_8c5137dd-355e-4dea-906a-9dfa2980381f.jsonl': 'https://sa5nmqnmqqzbyzq.blob.core.windows.net/azureml/ExperimentRun/dcid.0c224c09-ef50-4143-a736-f568e0554edd/logs/azureml/dataprep/python_span_8c5137dd-355e-4dea-906a-9dfa2980381f.jsonl?sv=2019-02-02&sr=b&sig=ePoCuQ0wTpyLk0mZ8dw%2BXGyQqusovrTW3LCDu%2FVitY4%3D&st=2020-12-05T07%3A14%3A46Z&se=2020-12-05T15%3A24%3A46Z&sp=r', 'logs/azureml/dataprep/python_span_9adca6ca-f8a7-45d2-9529-53df032d8068.jsonl': 'https://sa5nmqnmqqzbyzq.blob.core.windows.net/azureml/ExperimentRun/dcid.0c224c09-ef50-4143-a736-f568e0554edd/logs/azureml/dataprep/python_span_9adca6ca-f8a7-45d2-9529-53df032d8068.jsonl?sv=2019-02-02&sr=b&sig=4Ogt0Wd0vUxtff4wIQZkNfWVj3BDp6Yvsou4xUdYAW4%3D&st=2020-12-05T07%3A14%3A46Z&se=2020-12-05T15%3A24%3A46Z&sp=r', 'logs/azureml/dataprep/python_span_ac3160cf-a3c1-424c-b841-2cab8836450e.jsonl': 'https://sa5nmqnmqqzbyzq.blob.core.windows.net/azureml/ExperimentRun/dcid.0c224c09-ef50-4143-a736-f568e0554edd/logs/azureml/dataprep/python_span_ac3160cf-a3c1-424c-b841-2cab8836450e.jsonl?sv=2019-02-02&sr=b&sig=jz58skCgydxMesWeFi2RftL%2By%2FJ8tXQqnFJZPyrM6zw%3D&st=2020-12-05T07%3A14%3A46Z&se=2020-12-05T15%3A24%3A46Z&sp=r', 'logs/azureml/dataprep/python_span_b0175c44-55fa-44c9-a93b-1c8044faa7a4.jsonl': 'https://sa5nmqnmqqzbyzq.blob.core.windows.net/azureml/ExperimentRun/dcid.0c224c09-ef50-4143-a736-f568e0554edd/logs/azureml/dataprep/python_span_b0175c44-55fa-44c9-a93b-1c8044faa7a4.jsonl?sv=2019-02-02&sr=b&sig=ZlxwndodY99jq6GrXmmIYBDXTI4eAWfQLhhmEi3boMc%3D&st=2020-12-05T07%3A14%3A46Z&se=2020-12-05T15%3A24%3A46Z&sp=r', 'logs/azureml/dataprep/python_span_b785ce17-0451-46aa-a621-c16ee57cc200.jsonl': 'https://sa5nmqnmqqzbyzq.blob.core.windows.net/azureml/ExperimentRun/dcid.0c224c09-ef50-4143-a736-f568e0554edd/logs/azureml/dataprep/python_span_b785ce17-0451-46aa-a621-c16ee57cc200.jsonl?sv=2019-02-02&sr=b&sig=BD9Oh5IZTK8y4Gt84X%2BDfnclrLA%2BBoJ1hl5njUep%2Bjo%3D&st=2020-12-05T07%3A14%3A46Z&se=2020-12-05T15%3A24%3A46Z&sp=r', 'logs/azureml/dataprep/python_span_b8cc25c8-b2d4-42de-b89a-f94305a9d4b5.jsonl': 'https://sa5nmqnmqqzbyzq.blob.core.windows.net/azureml/ExperimentRun/dcid.0c224c09-ef50-4143-a736-f568e0554edd/logs/azureml/dataprep/python_span_b8cc25c8-b2d4-42de-b89a-f94305a9d4b5.jsonl?sv=2019-02-02&sr=b&sig=dkPL%2BJlL1W179kqwcE0%2FD5Lx6yUhVvyXWq4184SUlwU%3D&st=2020-12-05T07%3A14%3A46Z&se=2020-12-05T15%3A24%3A46Z&sp=r', 'logs/azureml/dataprep/python_span_f5223a3e-e1c3-4b5c-a7b6-11aa66d111f3.jsonl': 'https://sa5nmqnmqqzbyzq.blob.core.windows.net/azureml/ExperimentRun/dcid.0c224c09-ef50-4143-a736-f568e0554edd/logs/azureml/dataprep/python_span_f5223a3e-e1c3-4b5c-a7b6-11aa66d111f3.jsonl?sv=2019-02-02&sr=b&sig=2aV3wrTL9v24%2FtFEe5tkw9mdJRXxW6wslnMr29IDB6E%3D&st=2020-12-05T07%3A14%3A46Z&se=2020-12-05T15%3A24%3A46Z&sp=r', 'logs/azureml/dataprep/python_span_fcc60824-2ac2-4a87-a605-abd2d1777567.jsonl': 'https://sa5nmqnmqqzbyzq.blob.core.windows.net/azureml/ExperimentRun/dcid.0c224c09-ef50-4143-a736-f568e0554edd/logs/azureml/dataprep/python_span_fcc60824-2ac2-4a87-a605-abd2d1777567.jsonl?sv=2019-02-02&sr=b&sig=cpo73i00DewMYraVzJCXyRAp0z0zLh0NEIhJsdnDnIo%3D&st=2020-12-05T07%3A14%3A46Z&se=2020-12-05T15%3A24%3A46Z&sp=r', 'logs/azureml/executionlogs.txt': 'https://sa5nmqnmqqzbyzq.blob.core.windows.net/azureml/ExperimentRun/dcid.0c224c09-ef50-4143-a736-f568e0554edd/logs/azureml/executionlogs.txt?sv=2019-02-02&sr=b&sig=V4eDpi1DG%2BSmkX7SHj7xCeVMN5B4mtHhr8y02sePHQQ%3D&st=2020-12-05T07%3A14%3A46Z&se=2020-12-05T15%3A24%3A46Z&sp=r', 'logs/azureml/job_prep_azureml.log': 'https://sa5nmqnmqqzbyzq.blob.core.windows.net/azureml/ExperimentRun/dcid.0c224c09-ef50-4143-a736-f568e0554edd/logs/azureml/job_prep_azureml.log?sv=2019-02-02&sr=b&sig=wAE9owdFEzKq%2FKsysMU3nVRSr0nzGTJ5qKmRxEq3wYE%3D&st=2020-12-05T07%3A14%3A46Z&se=2020-12-05T15%3A24%3A46Z&sp=r', 'logs/azureml/job_release_azureml.log': 'https://sa5nmqnmqqzbyzq.blob.core.windows.net/azureml/ExperimentRun/dcid.0c224c09-ef50-4143-a736-f568e0554edd/logs/azureml/job_release_azureml.log?sv=2019-02-02&sr=b&sig=yI%2BINr%2FmBROBL2chYklux8UXNtGGEgSeAQWzlJ3N3Rk%3D&st=2020-12-05T07%3A14%3A46Z&se=2020-12-05T15%3A24%3A46Z&sp=r', 'logs/azureml/stderrlogs.txt': 'https://sa5nmqnmqqzbyzq.blob.core.windows.net/azureml/ExperimentRun/dcid.0c224c09-ef50-4143-a736-f568e0554edd/logs/azureml/stderrlogs.txt?sv=2019-02-02&sr=b&sig=Uqt5R7NKhKcJLNNiOBLSZcAuqy8W2E8ORpxoLy9BsuA%3D&st=2020-12-05T07%3A14%3A46Z&se=2020-12-05T15%3A24%3A46Z&sp=r', 'logs/azureml/stdoutlogs.txt': 'https://sa5nmqnmqqzbyzq.blob.core.windows.net/azureml/ExperimentRun/dcid.0c224c09-ef50-4143-a736-f568e0554edd/logs/azureml/stdoutlogs.txt?sv=2019-02-02&sr=b&sig=ukqfMJ1SWrPDVsN%2Bd7VarAysYUnDUTHFZIV736fsi%2Bc%3D&st=2020-12-05T07%3A14%3A46Z&se=2020-12-05T15%3A24%3A46Z&sp=r'}}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "StepRunId: 2dc2740f-5e11-4ba6-93c8-0fb62de6080d\n",
      "Link to Azure Machine Learning Portal: https://ml.azure.com/experiments/unified-dataprep-pipeline/runs/2dc2740f-5e11-4ba6-93c8-0fb62de6080d?wsid=/subscriptions/f3e38aaa-dd9c-4f17-95c1-ef3ff472da61/resourcegroups/nwk-aml-nov20-rg/workspaces/nwk-aml-nov20\n",
      "StepRun( write-prediction-data ) Status: NotStarted\n",
      "StepRun( write-prediction-data ) Status: Running\n",
      "\n",
      "Streaming azureml-logs/55_azureml-execution-tvmps_00898bac59e20e816df41ed0d006505b91eca395bbcbdc88c7dc39fbe5ce6204_d.txt\n",
      "========================================================================================================================\n",
      "2020-12-05T07:25:21Z Starting output-watcher...\n",
      "2020-12-05T07:25:21Z IsDedicatedCompute == True, won't poll for Low Pri Preemption\n",
      "2020-12-05T07:25:21Z Executing 'Copy ACR Details file' on 10.0.0.4\n",
      "2020-12-05T07:25:21Z Copy ACR Details file succeeded on 10.0.0.4. Output: \n",
      ">>>   \n",
      ">>>   \n",
      "Login Succeeded\n",
      "Using default tag: latest\n",
      "latest: Pulling from azureml/azureml_1a31af1e7a24bf75a2aeeda4db33c10b\n",
      "8e097b52bfb8: Already exists\n",
      "a613a9b4553c: Already exists\n",
      "acc000f01536: Already exists\n",
      "73eef93b7466: Already exists\n",
      "d5a54c1fb97f: Already exists\n",
      "1536f6ca931b: Already exists\n",
      "d7b631d130cb: Already exists\n",
      "75ffe8dfb222: Already exists\n",
      "86b4bf2f8d5f: Already exists\n",
      "5335952fa8d3: Already exists\n",
      "96fa3cc6fe10: Already exists\n",
      "e428dd9daa94: Already exists\n",
      "2ac8136f9128: Pulling fs layer\n",
      "3859aed457e6: Pulling fs layer\n",
      "4a3dff414402: Pulling fs layer\n",
      "400113670665: Pulling fs layer\n",
      "582ac19d3842: Pulling fs layer\n",
      "10f2d1363c3f: Pulling fs layer\n",
      "400113670665: Waiting\n",
      "582ac19d3842: Waiting\n",
      "10f2d1363c3f: Waiting\n",
      "2ac8136f9128: Verifying Checksum\n",
      "2ac8136f9128: Pull complete\n",
      "4a3dff414402: Download complete\n",
      "3859aed457e6: Verifying Checksum\n",
      "3859aed457e6: Download complete\n",
      "3859aed457e6: Pull complete\n",
      "4a3dff414402: Pull complete\n",
      "400113670665: Download complete\n",
      "10f2d1363c3f: Verifying Checksum\n",
      "10f2d1363c3f: Download complete\n",
      "400113670665: Pull complete\n",
      "582ac19d3842: Verifying Checksum\n",
      "582ac19d3842: Download complete\n",
      "582ac19d3842: Pull complete\n",
      "10f2d1363c3f: Pull complete\n",
      "Digest: sha256:a66506b217fc948965b0a828511e6d0a22fd62994639da253bf8e66e38a8fd1f\n",
      "Status: Downloaded newer image for 90700d52202e41a2a5ea297b8d0ebff8.azurecr.io/azureml/azureml_1a31af1e7a24bf75a2aeeda4db33c10b:latest\n",
      "90700d52202e41a2a5ea297b8d0ebff8.azurecr.io/azureml/azureml_1a31af1e7a24bf75a2aeeda4db33c10b:latest\n",
      "2020-12-05T07:25:31Z Check if container 2dc2740f-5e11-4ba6-93c8-0fb62de6080d already exist exited with 0, \n",
      "\n",
      "a0e5c6f26ffc95508e93c195adf5f224a169db6a69d7cd9f69920de1592c3b0d\n",
      "2020/12/05 07:25:35 Starting App Insight Logger for task:  containerSetup\n",
      "2020/12/05 07:25:35 Version: 3.0.01418.0008 Branch: .SourceBranch Commit: a8b24b5\n",
      "2020/12/05 07:25:35 /dev/infiniband/uverbs0 found (implying presence of InfiniBand)?: false\n",
      "2020/12/05 07:25:35 /dev/infiniband/uverbs0 found (implying presence of InfiniBand)?: false\n",
      "2020/12/05 07:25:35 sshd inside container not required for job, skipping setup.\n",
      "2020/12/05 07:25:35 All App Insights Logs was send successfully\n",
      "2020-12-05T07:25:35Z Starting docker container succeeded.\n",
      "\n",
      "Streaming azureml-logs/75_job_post-tvmps_00898bac59e20e816df41ed0d006505b91eca395bbcbdc88c7dc39fbe5ce6204_d.txt\n",
      "===============================================================================================================\n",
      "[2020-12-05T07:25:47.642550] Entering job release\n",
      "[2020-12-05T07:25:48.648595] Starting job release\n",
      "[2020-12-05T07:25:48.653239] Logging experiment finalizing status in history service.\n",
      "Starting the daemon thread to refresh tokens in background for process with pid = 152\n",
      "[2020-12-05T07:25:48.653514] job release stage : upload_datastore starting...\n",
      "[2020-12-05T07:25:48.656144] job release stage : start importing azureml.history._tracking in run_history_release.\n",
      "[2020-12-05T07:25:48.656191] job release stage : execute_job_release starting...\n",
      "[2020-12-05T07:25:48.663937] job release stage : copy_batchai_cached_logs starting...\n",
      "[2020-12-05T07:25:48.663990] job release stage : copy_batchai_cached_logs completed...\n",
      "[2020-12-05T07:25:48.665312] Entering context manager injector.\n",
      "[2020-12-05T07:25:48.915778] job release stage : send_run_telemetry starting...\n",
      "[2020-12-05T07:25:48.954571] job release stage : upload_datastore completed...\n",
      "[2020-12-05T07:25:49.110189] job release stage : execute_job_release completed...\n",
      "[2020-12-05T07:25:50.542079] job release stage : send_run_telemetry completed...\n",
      "[2020-12-05T07:25:50.542407] Job release is complete\n",
      "\n",
      "StepRun(write-prediction-data) Execution Summary\n",
      "=================================================\n",
      "StepRun( write-prediction-data ) Status: Finished\n",
      "{'runId': '2dc2740f-5e11-4ba6-93c8-0fb62de6080d', 'target': 'cpucluster', 'status': 'Completed', 'startTimeUtc': '2020-12-05T07:25:20.450175Z', 'endTimeUtc': '2020-12-05T07:25:58.121207Z', 'properties': {'azureml.runsource': 'azureml.StepRun', 'ContentSnapshotId': 'b5b607ec-b918-43cd-a8fd-91db5afc17a5', 'StepType': 'PythonScriptStep', 'ComputeTargetType': 'AmlCompute', 'azureml.moduleid': 'f3377b38-0e23-480d-a220-0e50bd4ae8da', 'azureml.nodeid': 'ab44b0b5', 'azureml.pipelinerunid': 'c3efe0ef-0eaf-48f1-8428-1df416c23116', '_azureml.ComputeTargetType': 'amlcompute', 'ProcessInfoFile': 'azureml-logs/process_info.json', 'ProcessStatusFile': 'azureml-logs/process_status.json'}, 'inputDatasets': [], 'outputDatasets': [], 'runDefinition': {'script': 'move_data.py', 'useAbsolutePath': False, 'arguments': ['--parallel_run_step_output', '$AZUREML_DATAREFERENCE_forecasting_output'], 'sourceDirectoryDataStore': None, 'framework': 'Python', 'communicator': 'None', 'target': 'cpucluster', 'dataReferences': {'forecasting_output': {'dataStoreName': 'workspaceblobstore', 'mode': 'Mount', 'pathOnDataStore': 'azureml/0c224c09-ef50-4143-a736-f568e0554edd/forecasting_output', 'pathOnCompute': None, 'overwrite': False}}, 'data': {}, 'outputData': {}, 'jobName': None, 'maxRunDurationSeconds': None, 'nodeCount': 1, 'priority': None, 'credentialPassthrough': False, 'environment': {'name': 'Experiment unified-dataprep-pipeline Environment', 'version': 'Autosave_2020-12-05T07:25:05Z_be0ffb0c', 'python': {'interpreterPath': 'python', 'userManagedDependencies': False, 'condaDependencies': {'channels': ['anaconda', 'conda-forge'], 'dependencies': ['python=3.6.2', {'pip': ['azureml-defaults']}], 'name': 'azureml_da3e97fcb51801118b8e80207f3e01ad'}, 'baseCondaEnvironment': None}, 'environmentVariables': {'EXAMPLE_ENV_VAR': 'EXAMPLE_VALUE'}, 'docker': {'baseImage': 'mcr.microsoft.com/azureml/intelmpi2018.3-ubuntu16.04:20200821.v1', 'platform': {'os': 'Linux', 'architecture': 'amd64'}, 'baseDockerfile': None, 'baseImageRegistry': {'address': None, 'username': None, 'password': None}, 'enabled': True, 'arguments': []}, 'spark': {'repositories': [], 'packages': [], 'precachePackages': True}, 'inferencingStackVersion': None}, 'history': {'outputCollection': True, 'directoriesToWatch': ['logs'], 'enableMLflowTracking': True, 'snapshotProject': True}, 'spark': {'configuration': {'spark.app.name': 'Azure ML Experiment', 'spark.yarn.maxAppAttempts': '1'}}, 'parallelTask': {'maxRetriesPerWorker': 0, 'workerCountPerNode': 1, 'terminalExitCodes': None, 'configuration': {}}, 'amlCompute': {'name': None, 'vmSize': None, 'retainCluster': False, 'clusterMaxNodeCount': 1}, 'aiSuperComputer': {'instanceType': None, 'frameworkImage': None, 'imageVersion': None, 'location': None}, 'tensorflow': {'workerCount': 1, 'parameterServerCount': 1}, 'mpi': {'processCountPerNode': 1}, 'hdi': {'yarnDeployMode': 'Cluster'}, 'containerInstance': {'region': None, 'cpuCores': 2.0, 'memoryGb': 3.5}, 'exposedPorts': None, 'docker': {'useDocker': True, 'sharedVolumes': True, 'shmSize': '2g', 'arguments': []}, 'cmk8sCompute': {'configuration': {}}}, 'logFiles': {'azureml-logs/55_azureml-execution-tvmps_00898bac59e20e816df41ed0d006505b91eca395bbcbdc88c7dc39fbe5ce6204_d.txt': 'https://sa5nmqnmqqzbyzq.blob.core.windows.net/azureml/ExperimentRun/dcid.2dc2740f-5e11-4ba6-93c8-0fb62de6080d/azureml-logs/55_azureml-execution-tvmps_00898bac59e20e816df41ed0d006505b91eca395bbcbdc88c7dc39fbe5ce6204_d.txt?sv=2019-02-02&sr=b&sig=UK1V%2FkmGYDiIsMwTg1PwYSZhSMhNvRc9rrB01MA71PU%3D&st=2020-12-05T07%3A15%3A52Z&se=2020-12-05T15%3A25%3A52Z&sp=r', 'azureml-logs/65_job_prep-tvmps_00898bac59e20e816df41ed0d006505b91eca395bbcbdc88c7dc39fbe5ce6204_d.txt': 'https://sa5nmqnmqqzbyzq.blob.core.windows.net/azureml/ExperimentRun/dcid.2dc2740f-5e11-4ba6-93c8-0fb62de6080d/azureml-logs/65_job_prep-tvmps_00898bac59e20e816df41ed0d006505b91eca395bbcbdc88c7dc39fbe5ce6204_d.txt?sv=2019-02-02&sr=b&sig=myP5%2BxhI1NeDku22ALQfbYXy0%2FC%2Fqj8qS8i%2BNN5kjvU%3D&st=2020-12-05T07%3A15%3A52Z&se=2020-12-05T15%3A25%3A52Z&sp=r', 'azureml-logs/70_driver_log.txt': 'https://sa5nmqnmqqzbyzq.blob.core.windows.net/azureml/ExperimentRun/dcid.2dc2740f-5e11-4ba6-93c8-0fb62de6080d/azureml-logs/70_driver_log.txt?sv=2019-02-02&sr=b&sig=QPN6Y0GaYzlTivU1Q%2FucgUXr6G8EU7fd1SeBe2YiYu0%3D&st=2020-12-05T07%3A15%3A52Z&se=2020-12-05T15%3A25%3A52Z&sp=r', 'azureml-logs/75_job_post-tvmps_00898bac59e20e816df41ed0d006505b91eca395bbcbdc88c7dc39fbe5ce6204_d.txt': 'https://sa5nmqnmqqzbyzq.blob.core.windows.net/azureml/ExperimentRun/dcid.2dc2740f-5e11-4ba6-93c8-0fb62de6080d/azureml-logs/75_job_post-tvmps_00898bac59e20e816df41ed0d006505b91eca395bbcbdc88c7dc39fbe5ce6204_d.txt?sv=2019-02-02&sr=b&sig=Et%2FPFqoPonF%2FJ%2FbyEOOMkjhiiPBncYWnAGPW1EbRsTw%3D&st=2020-12-05T07%3A15%3A52Z&se=2020-12-05T15%3A25%3A52Z&sp=r', 'azureml-logs/process_info.json': 'https://sa5nmqnmqqzbyzq.blob.core.windows.net/azureml/ExperimentRun/dcid.2dc2740f-5e11-4ba6-93c8-0fb62de6080d/azureml-logs/process_info.json?sv=2019-02-02&sr=b&sig=rNe4JOh550VxSiB0LDEXhQfTT0vyWCh8KUqjwjVZGBg%3D&st=2020-12-05T07%3A15%3A52Z&se=2020-12-05T15%3A25%3A52Z&sp=r', 'azureml-logs/process_status.json': 'https://sa5nmqnmqqzbyzq.blob.core.windows.net/azureml/ExperimentRun/dcid.2dc2740f-5e11-4ba6-93c8-0fb62de6080d/azureml-logs/process_status.json?sv=2019-02-02&sr=b&sig=4WYeHEi1Ysb2Mh%2F6keb98BS08vJRvEvewVyZq38ueYw%3D&st=2020-12-05T07%3A15%3A52Z&se=2020-12-05T15%3A25%3A52Z&sp=r', 'logs/azureml/114_azureml.log': 'https://sa5nmqnmqqzbyzq.blob.core.windows.net/azureml/ExperimentRun/dcid.2dc2740f-5e11-4ba6-93c8-0fb62de6080d/logs/azureml/114_azureml.log?sv=2019-02-02&sr=b&sig=h5TMyka57j%2BAmyE3d1em80Q0AATkPgOG%2BNIuGajJK4g%3D&st=2020-12-05T07%3A15%3A51Z&se=2020-12-05T15%3A25%3A51Z&sp=r', 'logs/azureml/executionlogs.txt': 'https://sa5nmqnmqqzbyzq.blob.core.windows.net/azureml/ExperimentRun/dcid.2dc2740f-5e11-4ba6-93c8-0fb62de6080d/logs/azureml/executionlogs.txt?sv=2019-02-02&sr=b&sig=09DD4gRTvANKUb46%2BrJ%2B1Q2jFSk7X48g3TbdAchiXM4%3D&st=2020-12-05T07%3A15%3A51Z&se=2020-12-05T15%3A25%3A51Z&sp=r', 'logs/azureml/job_prep_azureml.log': 'https://sa5nmqnmqqzbyzq.blob.core.windows.net/azureml/ExperimentRun/dcid.2dc2740f-5e11-4ba6-93c8-0fb62de6080d/logs/azureml/job_prep_azureml.log?sv=2019-02-02&sr=b&sig=JghccW0h%2BzYvbHeS3KR6lziKVmbXsmiUGhdf2AbJ5ho%3D&st=2020-12-05T07%3A15%3A51Z&se=2020-12-05T15%3A25%3A51Z&sp=r', 'logs/azureml/job_release_azureml.log': 'https://sa5nmqnmqqzbyzq.blob.core.windows.net/azureml/ExperimentRun/dcid.2dc2740f-5e11-4ba6-93c8-0fb62de6080d/logs/azureml/job_release_azureml.log?sv=2019-02-02&sr=b&sig=r09GktQ%2BT3zBb5hrQqh685Nf%2BusxNwgOdLhdoSIQ6o8%3D&st=2020-12-05T07%3A15%3A51Z&se=2020-12-05T15%3A25%3A51Z&sp=r', 'logs/azureml/stderrlogs.txt': 'https://sa5nmqnmqqzbyzq.blob.core.windows.net/azureml/ExperimentRun/dcid.2dc2740f-5e11-4ba6-93c8-0fb62de6080d/logs/azureml/stderrlogs.txt?sv=2019-02-02&sr=b&sig=viBgJwY2Z8LIimCY39cRzfp4djU1qn9%2BIZgmeIaGz8I%3D&st=2020-12-05T07%3A15%3A51Z&se=2020-12-05T15%3A25%3A51Z&sp=r', 'logs/azureml/stdoutlogs.txt': 'https://sa5nmqnmqqzbyzq.blob.core.windows.net/azureml/ExperimentRun/dcid.2dc2740f-5e11-4ba6-93c8-0fb62de6080d/logs/azureml/stdoutlogs.txt?sv=2019-02-02&sr=b&sig=pGq2jqhYIM4P05NzsADW1n3NhIbjqwiP8kqb19RLwvs%3D&st=2020-12-05T07%3A15%3A51Z&se=2020-12-05T15%3A25%3A51Z&sp=r'}}\n",
      "\n",
      "\n",
      "\n",
      "PipelineRun Execution Summary\n",
      "==============================\n",
      "PipelineRun Status: Finished\n",
      "{'runId': 'c3efe0ef-0eaf-48f1-8428-1df416c23116', 'status': 'Completed', 'startTimeUtc': '2020-12-05T06:52:20.48286Z', 'endTimeUtc': '2020-12-05T07:26:05.577525Z', 'properties': {'azureml.runsource': 'azureml.PipelineRun', 'runSource': 'SDK', 'runType': 'SDK', 'azureml.parameters': '{}'}, 'inputDatasets': [], 'logFiles': {'logs/azureml/executionlogs.txt': 'https://sa5nmqnmqqzbyzq.blob.core.windows.net/azureml/ExperimentRun/dcid.c3efe0ef-0eaf-48f1-8428-1df416c23116/logs/azureml/executionlogs.txt?sv=2019-02-02&sr=b&sig=QKjvYqhNoGHv2kIle1810Tbcsz5xsaIY6FsjeBtaAY0%3D&st=2020-12-05T06%3A43%3A03Z&se=2020-12-05T14%3A53%3A03Z&sp=r', 'logs/azureml/stderrlogs.txt': 'https://sa5nmqnmqqzbyzq.blob.core.windows.net/azureml/ExperimentRun/dcid.c3efe0ef-0eaf-48f1-8428-1df416c23116/logs/azureml/stderrlogs.txt?sv=2019-02-02&sr=b&sig=2j15FwX27ZBj6PutnNzwbw3WJB0UNpLDgaICe2FO8JQ%3D&st=2020-12-05T06%3A43%3A03Z&se=2020-12-05T14%3A53%3A03Z&sp=r', 'logs/azureml/stdoutlogs.txt': 'https://sa5nmqnmqqzbyzq.blob.core.windows.net/azureml/ExperimentRun/dcid.c3efe0ef-0eaf-48f1-8428-1df416c23116/logs/azureml/stdoutlogs.txt?sv=2019-02-02&sr=b&sig=2q00Bqa%2B2D41I7O%2B6b5SZtvD8Ol%2BZIysWrcHe9KhloA%3D&st=2020-12-05T06%3A43%3A03Z&se=2020-12-05T14%3A53%3A03Z&sp=r'}}\n",
      "\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'Finished'"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "# #Create pipeline, execute pipeline, and wait for response\n",
    "pipeline = Pipeline(workspace=ws, steps=[aggregate_data_step, train_parallel_run_step, inference_parallel_run_step, copy_data_step])\n",
    "run = experiment.submit(pipeline)\n",
    "run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Publish pipeline to endpoint\n",
    "published_pipeline = pipeline.publish(name = 'many_models_sample',\n",
    "                                     description = 'Gathers data, trains models, generates forecasts, and stores results',\n",
    "                                     version = '1',\n",
    "                                     continue_on_step_failure = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sample remote execution\n",
    "#Pipeline execution via REST endpoint requires AAD Token (obtained here from service principal)\n",
    "#Relevant docs:\n",
    "#https://docs.microsoft.com/en-us/azure/machine-learning/how-to-deploy-pipelines\n",
    "#https://github.com/Azure/MachineLearningNotebooks/blob/master/how-to-use-azureml/manage-azureml-service/authentication-in-azureml/authentication-in-azureml.ipynb\n",
    "\n",
    "import requests\n",
    "import os\n",
    "from azureml.core.authentication import ServicePrincipalAuthentication\n",
    "\n",
    "#Service principal creds stored as environment vars\n",
    "client_id = os.environ.get('client_id')\n",
    "tenant_id = os.environ.get('tenant_id')\n",
    "service_principal_password = os.environ.get('service_principal_password')\n",
    "\n",
    "#Leverage ADAL library for obtaining token\n",
    "from adal import AuthenticationContext\n",
    "\n",
    "client_id = client_id\n",
    "client_secret = service_principal_password\n",
    "resource_url = \"https://login.microsoftonline.com\"\n",
    "tenant_id = tenant_id\n",
    "authority = \"{}/{}\".format(resource_url, tenant_id)\n",
    "\n",
    "auth_context = AuthenticationContext(authority)\n",
    "token_response = auth_context.acquire_token_with_client_credentials(\"https://management.azure.com/\", client_id, client_secret)\n",
    "\n",
    "#Format token response for API request to pipeline\n",
    "headers = {'Authorization': 'Bearer {}'.format(token_response['accessToken'])}\n",
    "\n",
    "#Trigger remote pipeline run\n",
    "#Pipeline endpoint can be obtained from AML portal as well\n",
    "response = requests.post(published_pipeline.endpoint,\n",
    "                         headers=headers,\n",
    "                         json={\"ExperimentName\": \"REST_Pipeline_Trigger_Test\",\n",
    "                               \"ParameterAssignments\": {\"target_path\": \"source_data_{}\".format(timestamp)}})"
   ]
  }
 ]
}